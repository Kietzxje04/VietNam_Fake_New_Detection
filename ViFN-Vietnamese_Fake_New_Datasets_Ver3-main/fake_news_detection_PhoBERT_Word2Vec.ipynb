{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14459,
     "status": "error",
     "timestamp": 1757222032173,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "OKxJUrWz7Lo0",
    "outputId": "e184dc2a-33e8-421d-db56-c7021d6fe4a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [21 lines of output]\n",
      "      + C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48\\vendored-meson\\meson\\meson.py setup C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48 C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48\\.mesonpy-33gjuebu -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48\\.mesonpy-33gjuebu\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48\n",
      "      Build dir: C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48\\.mesonpy-33gjuebu\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-vu3sowq7\\numpy_a53ab3e991524890a2f012c7d5ef5b48\\.mesonpy-33gjuebu\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "√ó Encountered error while generating package metadata.\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [21 lines of output]\n",
      "      + C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab\\vendored-meson\\meson\\meson.py setup C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab\\.mesonpy-8gsjfy1p -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab\\.mesonpy-8gsjfy1p\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab\n",
      "      Build dir: C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab\\.mesonpy-8gsjfy1p\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Kiet\\AppData\\Local\\Temp\\pip-install-6eeh4cyq\\numpy_e35952589f69450da10c697ac458d1ab\\.mesonpy-8gsjfy1p\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "√ó Encountered error while generating package metadata.\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing Vietnamese NLP packages...\n",
      "‚úÖ pyvi installed successfully\n",
      "‚ö†Ô∏è Failed to install underthesea: Command '['C:\\\\Users\\\\Kiet\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\\\python.exe', '-m', 'pip', 'install', 'underthesea', '-q']' returned non-zero exit status 2.\n",
      "   Will use alternative tokenization\n",
      "‚úÖ wordcloud installed successfully\n",
      "\n",
      "‚úÖ Core packages installed successfully!\n",
      "üìä PyVi available: True\n",
      "üìä Underthesea available: False\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üì¶ CELL 1: INSTALL PACKAGES (FIXED VERSION)\n",
    "# ===============================\n",
    "# Install core packages first\n",
    "!pip install transformers datasets torch scikit-learn matplotlib seaborn tqdm gensim nltk -q\n",
    "# Fix numpy compatibility\n",
    "# Force reinstall numpy + scikit-learn stable versions\n",
    "!pip install --upgrade --force-reinstall numpy==1.26.4 scikit-learn==1.3.2 -q\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import subprocess, sys\n",
    "\n",
    "def ensure_numpy_compat():\n",
    "    import numpy as np\n",
    "    version = tuple(map(int, np.__version__.split(\".\")[:2]))\n",
    "    if version >= (2,0):\n",
    "        print(f\"‚ö†Ô∏è Detected numpy {np.__version__}, downgrading to 1.26.4 for compatibility...\")\n",
    "        # Use Jupyter magic command for compatibility\n",
    "        get_ipython().run_line_magic('pip', 'install numpy==1.26.4 --upgrade --force-reinstall')\n",
    "        print(\"‚úÖ Numpy fixed. Please restart the runtime now!\")\n",
    "\n",
    "# Install Vietnamese NLP packages with error handling\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_with_fallback(package_name, fallback_msg=\"\"):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"-q\"])\n",
    "        print(f\"‚úÖ {package_name} installed successfully\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to install {package_name}: {e}\")\n",
    "        if fallback_msg:\n",
    "            print(f\"   {fallback_msg}\")\n",
    "        return False\n",
    "\n",
    "# Try to install Vietnamese packages\n",
    "print(\"üì¶ Installing Vietnamese NLP packages...\")\n",
    "pyvi_available = install_with_fallback(\"pyvi\", \"Will use basic tokenization instead\")\n",
    "underthesea_available = install_with_fallback(\"underthesea\", \"Will use alternative tokenization\")\n",
    "\n",
    "# Optional packages\n",
    "install_with_fallback(\"wordcloud\", \"Word cloud generation will be skipped\")\n",
    "\n",
    "print(\"\\n‚úÖ Core packages installed successfully!\")\n",
    "print(f\"üìä PyVi available: {pyvi_available}\")\n",
    "print(f\"üìä Underthesea available: {underthesea_available}\")\n",
    "\n",
    "# Alternative minimal installation if Vietnamese packages fail\n",
    "if not pyvi_available and not underthesea_available:\n",
    "    print(\"\\n‚ö†Ô∏è Vietnamese tokenizers not available - using basic preprocessing\")\n",
    "    print(\"This will still work but with reduced Vietnamese text processing quality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14575,
     "status": "aborted",
     "timestamp": 1757222032180,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "yYB8AhHucQEM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üî• Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory: 6.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Kiet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall datasets -q\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "%pip install transformers -q\n",
    "%pip install tf-keras -q\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback, AutoConfig\n",
    ")\n",
    "%pip install datasets -q\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Vietnamese text processing\n",
    "try:\n",
    "    from pyvi import ViTokenizer\n",
    "    PYVI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyVi not available, will use basic tokenization\")\n",
    "    PYVI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from underthesea import word_tokenize\n",
    "    UNDERTHESEA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Underthesea not available, will use alternative tokenization\")\n",
    "    UNDERTHESEA_AVAILABLE = False\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14573,
     "status": "aborted",
     "timestamp": 1757222032181,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "4HhlH8FyQ1s8"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üîß CELL 3: ADVANCED VIETNAMESE TEXT PREPROCESSOR\n",
    "# ===============================\n",
    "class AdvancedVietnameseTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.html_tags = re.compile('<.*?>')\n",
    "\n",
    "        # Vietnamese stopwords (expanded list)\n",
    "        self.vietnamese_stopwords = {\n",
    "            'v√†', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'n√†y', 'ƒë√≥', 'c√°c', 'm·ªôt', 'kh√¥ng', 'ƒë·ªÉ', 'trong',\n",
    "            'c·ªßa', 'v·ªõi', 'v·ªÅ', 't·ª´', 'theo', 'nh∆∞', 'tr√™n', 'd∆∞·ªõi', 'sau', 'tr∆∞·ªõc',\n",
    "            'ƒë√£', 's·∫Ω', 'ƒëang', 'b·ªã', 'cho', 't·∫°i', 'do', 'v√¨', 'n√™n', 'm√†', 'hay',\n",
    "            'ho·∫∑c', 'nh∆∞ng', 'tuy', 'd√π', 'n·∫øu', 'khi', 'l√∫c', 'b√¢y_gi·ªù', 'hi·ªán_t·∫°i',\n",
    "            'ng√†y', 'th√°ng', 'nƒÉm', 'gi·ªù', 'ph√∫t', 'gi√¢y', 'r·ªìi', 'ƒë√¢y', 'kia'\n",
    "        }\n",
    "\n",
    "        # Common replacements for text normalization\n",
    "        self.replacements = {\n",
    "            # Number normalization\n",
    "            r'\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}': ' <DATE> ',  # dates\n",
    "            r'\\d+[.,]\\d+': ' <NUMBER> ',  # decimal numbers\n",
    "            r'\\d+': ' <NUMBER> ',  # integers\n",
    "\n",
    "            # Special characters normalization\n",
    "            r'[!]{2,}': ' <EXCLAMATION> ',\n",
    "            r'[?]{2,}': ' <QUESTION> ',\n",
    "            r'[.]{3,}': ' <DOTS> ',\n",
    "\n",
    "            # Repeated characters\n",
    "            r'(.)\\1{2,}': r'\\1\\1',  # reduce repeated chars to max 2\n",
    "        }\n",
    "\n",
    "        # Common fake news indicators in Vietnamese\n",
    "        self.fake_indicators = {\n",
    "            'n√≥ng', 'hot', 'shock', 'kh·∫©n_c·∫•p', 'c·∫£nh_b√°o', 'nguy_hi·ªÉm',\n",
    "            'b√≠_m·∫≠t', 'ti·∫øt_l·ªô', 'ph√°t_hi·ªán', 'ƒë·ªôt_ph√°', '100%', 'ch·∫Øc_ch·∫Øn',\n",
    "            'tuy·ªát_ƒë·ªëi', 'kh√¥ng_bao_gi·ªù', 'lu√¥n_lu√¥n', 'm√£i_m√£i'\n",
    "        }\n",
    "\n",
    "    def clean_html(self, text):\n",
    "        \"\"\"Advanced HTML cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Decode HTML entities\n",
    "        text = html.unescape(str(text))\n",
    "\n",
    "        # Remove HTML tags but keep some structure\n",
    "        text = self.html_tags.sub(' ', text)\n",
    "\n",
    "        # Remove script and style content\n",
    "        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        \"\"\"Advanced text normalization\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Apply replacements\n",
    "        for pattern, replacement in self.replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "\n",
    "        # Remove URLs and emails\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' <URL> ', text)\n",
    "        text = re.sub(r'\\S+@\\S+', ' <EMAIL> ', text)\n",
    "\n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'(\\+84|0)[0-9]{9,10}', ' <PHONE> ', text)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize_vietnamese(self, text):\n",
    "        \"\"\"Vietnamese-aware tokenization\"\"\"\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return []\n",
    "\n",
    "        # Use Vietnamese tokenizer if available\n",
    "        if UNDERTHESEA_AVAILABLE:\n",
    "            try:\n",
    "                tokens = word_tokenize(text)\n",
    "                return [token for token in tokens if len(token) > 1]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if PYVI_AVAILABLE:\n",
    "            try:\n",
    "                tokenized = ViTokenizer.tokenize(text)\n",
    "                return [token for token in tokenized.split() if len(token) > 1]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Fallback to simple tokenization\n",
    "        return simple_preprocess(text, min_len=2, max_len=50)\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        return [token for token in tokens if token not in self.vietnamese_stopwords]\n",
    "\n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract linguistic features\"\"\"\n",
    "        features = {\n",
    "            'length': len(str(text)),\n",
    "            'word_count': len(str(text).split()),\n",
    "            'avg_word_length': np.mean([len(word) for word in str(text).split()]) if str(text).split() else 0,\n",
    "            'exclamation_count': str(text).count('!'),\n",
    "            'question_count': str(text).count('?'),\n",
    "            'uppercase_ratio': sum(1 for c in str(text) if c.isupper()) / len(str(text)) if str(text) else 0,\n",
    "            'digit_count': sum(1 for c in str(text) if c.isdigit()),\n",
    "            'fake_indicator_count': sum(1 for word in str(text).lower().split() if word in self.fake_indicators)\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def clean_and_process(self, text, remove_stopwords=False, extract_features=False):\n",
    "        \"\"\"Complete text processing pipeline\"\"\"\n",
    "        # Clean HTML\n",
    "        clean_text = self.clean_html(text)\n",
    "\n",
    "        # Normalize\n",
    "        normalized = self.normalize_text(clean_text)\n",
    "\n",
    "        # Extract features if requested\n",
    "        features = self.extract_features(normalized) if extract_features else None\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = self.tokenize_vietnamese(normalized)\n",
    "\n",
    "        # Remove stopwords if requested\n",
    "        if remove_stopwords:\n",
    "            tokens = self.remove_stopwords(tokens)\n",
    "\n",
    "        processed_text = ' '.join(tokens)\n",
    "\n",
    "        return processed_text, features\n",
    "\n",
    "print(\"‚úÖ Advanced Vietnamese Text Preprocessor defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:21:33.755817Z",
     "iopub.status.busy": "2024-11-02T07:21:33.755136Z",
     "iopub.status.idle": "2024-11-02T07:21:36.001764Z",
     "shell.execute_reply": "2024-11-02T07:21:36.000854Z",
     "shell.execute_reply.started": "2024-11-02T07:21:33.755774Z"
    },
    "executionInfo": {
     "elapsed": 14572,
     "status": "aborted",
     "timestamp": 1757222032183,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "k5MoorOlYSiO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üìã CELL 4: ENHANCED DATASET PREPROCESSOR\n",
    "# ===============================\n",
    "class EnhancedNewsDatasetPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.text_preprocessor = AdvancedVietnameseTextPreprocessor()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "    def combine_text_features(self, df, use_weights=True):\n",
    "        \"\"\"Enhanced text combination with weights\"\"\"\n",
    "        combined_texts = []\n",
    "        linguistic_features = []\n",
    "\n",
    "        # Weights for different text components\n",
    "        weights = {\n",
    "            'title': 3.0,      # Title is most important\n",
    "            'summary': 2.0,    # Summary is very important\n",
    "            'content': 1.0     # Content is baseline\n",
    "        }\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing texts\"):\n",
    "            text_parts = []\n",
    "            row_features = {}\n",
    "\n",
    "            # Process title\n",
    "            if pd.notna(row.get('title')) and str(row.get('title')).strip():\n",
    "                title_clean, title_feat = self.text_preprocessor.clean_and_process(\n",
    "                    row['title'], remove_stopwords=False, extract_features=True\n",
    "                )\n",
    "                if title_clean:\n",
    "                    if use_weights:\n",
    "                        text_parts.extend([title_clean] * int(weights['title']))\n",
    "                    else:\n",
    "                        text_parts.append(title_clean)\n",
    "                    row_features.update({f'title_{k}': v for k, v in title_feat.items()})\n",
    "\n",
    "            # Process summary\n",
    "            if pd.notna(row.get('summary')) and str(row.get('summary')).strip():\n",
    "                summary_clean, summary_feat = self.text_preprocessor.clean_and_process(\n",
    "                    row['summary'], remove_stopwords=False, extract_features=True\n",
    "                )\n",
    "                if summary_clean:\n",
    "                    if use_weights:\n",
    "                        text_parts.extend([summary_clean] * int(weights['summary']))\n",
    "                    else:\n",
    "                        text_parts.append(summary_clean)\n",
    "                    row_features.update({f'summary_{k}': v for k, v in summary_feat.items()})\n",
    "\n",
    "            # Process content\n",
    "            if pd.notna(row.get('content_html')) and str(row.get('content_html')).strip():\n",
    "                content_clean, content_feat = self.text_preprocessor.clean_and_process(\n",
    "                    row['content_html'], remove_stopwords=False, extract_features=True\n",
    "                )\n",
    "                if content_clean:\n",
    "                    text_parts.append(content_clean)\n",
    "                    row_features.update({f'content_{k}': v for k, v in content_feat.items()})\n",
    "\n",
    "            # Combine texts\n",
    "            combined_text = ' [SEP] '.join(text_parts) if text_parts else \"\"\n",
    "            combined_texts.append(combined_text)\n",
    "            linguistic_features.append(row_features)\n",
    "\n",
    "        return combined_texts, linguistic_features\n",
    "\n",
    "    def prepare_data(self, df, min_length=50, max_length=8000, balance_data=False):\n",
    "        \"\"\"Enhanced data preparation with optional balancing\"\"\"\n",
    "        print(\"üìã Enhanced preprocessing...\")\n",
    "\n",
    "        # Combine texts and extract features\n",
    "        combined_texts, linguistic_features = self.combine_text_features(df)\n",
    "        df['combined_text'] = combined_texts\n",
    "\n",
    "        # Add linguistic features to dataframe\n",
    "        if linguistic_features and linguistic_features[0]:  # Check if features exist\n",
    "            feature_df = pd.DataFrame(linguistic_features)\n",
    "            df = pd.concat([df, feature_df], axis=1)\n",
    "\n",
    "        # Filter by text length\n",
    "        df['text_length'] = df['combined_text'].str.len()\n",
    "\n",
    "        print(f\"üìä Original dataset: {len(df)} samples\")\n",
    "\n",
    "        # Remove texts that are too short or too long\n",
    "        df = df[(df['text_length'] >= min_length) & (df['text_length'] <= max_length)]\n",
    "\n",
    "        # Remove empty texts\n",
    "        df = df[df['combined_text'].str.strip() != '']\n",
    "\n",
    "        print(f\"üìä After length filtering: {len(df)} samples\")\n",
    "\n",
    "        # Balance dataset if requested\n",
    "        if balance_data:\n",
    "            df = self.balance_dataset(df)\n",
    "\n",
    "        # Ensure labels are binary\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "        print(f\"üìä Final dataset: {len(df)} samples\")\n",
    "        print(f\"üìä Label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "        print(f\"üìä Text length statistics:\")\n",
    "        print(df['text_length'].describe())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def balance_dataset(self, df):\n",
    "        \"\"\"Balance dataset using undersampling\"\"\"\n",
    "        print(\"‚öñÔ∏è Balancing dataset...\")\n",
    "\n",
    "        # Get minority class size\n",
    "        label_counts = df['label'].value_counts()\n",
    "        min_size = label_counts.min()\n",
    "\n",
    "        # Sample equal amounts from each class\n",
    "        balanced_dfs = []\n",
    "        for label in df['label'].unique():\n",
    "            class_df = df[df['label'] == label].sample(n=min_size, random_state=42)\n",
    "            balanced_dfs.append(class_df)\n",
    "\n",
    "        balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "        print(f\"üìä Balanced dataset: {len(balanced_df)} samples\")\n",
    "        print(f\"üìä New label distribution:\")\n",
    "        print(balanced_df['label'].value_counts())\n",
    "\n",
    "        return balanced_df\n",
    "\n",
    "print(\"‚úÖ Enhanced Dataset Preprocessor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:21:39.872886Z",
     "iopub.status.busy": "2024-11-02T07:21:39.872125Z",
     "iopub.status.idle": "2024-11-02T07:21:39.887561Z",
     "shell.execute_reply": "2024-11-02T07:21:39.886649Z",
     "shell.execute_reply.started": "2024-11-02T07:21:39.872844Z"
    },
    "executionInfo": {
     "elapsed": 14571,
     "status": "aborted",
     "timestamp": 1757222032184,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "pjB09qYPYSiR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 5: ü§ñ UNIVERSAL TRANSFORMER CLASSIFIER (FIXED INDENTATION)\n",
    "# ===============================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class UniversalTransformerClassifier:\n",
    "\n",
    "    def __init__(self, model_name: str, max_length: int = 512, num_labels: int = 2, from_scratch: bool = False):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.num_labels = num_labels\n",
    "        self.from_scratch = from_scratch\n",
    "\n",
    "        print(f\"\\nü§ñ Initializing {model_name} (from_scratch={from_scratch}) ...\")\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "        # model\n",
    "        if from_scratch:\n",
    "            config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "            self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "            print(f\"‚ö†Ô∏è Model {model_name} initialized with RANDOM weights.\")\n",
    "        else:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "            print(f\"‚úÖ Model {model_name} loaded pretrained weights.\")\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.history = None\n",
    "        self.best_epoch = None\n",
    "\n",
    "    def encode(self, texts, labels=None):\n",
    "        \"\"\"Tokenize list[str] -> TensorDataset.\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        if labels is not None:\n",
    "            labels_arr = torch.tensor(labels, dtype=torch.long)\n",
    "            return TensorDataset(input_ids, attention_mask, labels_arr)\n",
    "        else:\n",
    "            return TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels,\n",
    "              num_epochs: int = 15,\n",
    "              batch_size: int = 16,\n",
    "              learning_rate: float = 2e-5,\n",
    "              weight_decay: float = 0.0,\n",
    "              save_best: bool = True,\n",
    "              best_save_dir: str = \"./best_model\"):\n",
    "\n",
    "        print(f\"\\nüöÄ Training {self.model_name} for {num_epochs} epochs (from_scratch={self.from_scratch})\")\n",
    "\n",
    "        # dataloaders\n",
    "        train_ds = self.encode(train_texts, train_labels)\n",
    "        val_ds   = self.encode(val_texts, val_labels)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_f1 = -1.0\n",
    "        best_state = None\n",
    "        best_preds, best_labels = None, None\n",
    "        saved_path = None\n",
    "        decrease_count = 0\n",
    "\n",
    "        # history\n",
    "        self.history = {\"train_loss\": [], \"val_acc\": [], \"val_prec\": [], \"val_rec\": [], \"val_f1\": [], \"val_auc\": []}\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            # ---- training ----\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\", leave=False):\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "\n",
    "            # ---- validation ----\n",
    "            self.model.eval()\n",
    "            val_preds, val_labels_all, val_probs = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} - Validation\", leave=False):\n",
    "                    input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    logits = outputs.logits\n",
    "                    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                    preds = np.argmax(probs, axis=1)\n",
    "\n",
    "                    val_preds.extend(preds.tolist())\n",
    "                    val_labels_all.extend(labels.cpu().numpy().tolist())\n",
    "                    val_probs.extend(probs.tolist())\n",
    "\n",
    "            # metrics\n",
    "            acc = accuracy_score(val_labels_all, val_preds)\n",
    "            prec = precision_score(val_labels_all, val_preds, zero_division=0)\n",
    "            rec = recall_score(val_labels_all, val_preds, zero_division=0)\n",
    "            f1 = f1_score(val_labels_all, val_preds, zero_division=0)\n",
    "            try:\n",
    "                auc = roc_auc_score(val_labels_all, np.array(val_probs)[:, 1])\n",
    "            except Exception:\n",
    "                auc = 0.0\n",
    "\n",
    "            # record\n",
    "            self.history[\"train_loss\"].append(avg_train_loss)\n",
    "            self.history[\"val_acc\"].append(acc)\n",
    "            self.history[\"val_prec\"].append(prec)\n",
    "            self.history[\"val_rec\"].append(rec)\n",
    "            self.history[\"val_f1\"].append(f1)\n",
    "            self.history[\"val_auc\"].append(auc)\n",
    "\n",
    "            # log\n",
    "            print(f\"üìä Epoch {epoch}/{num_epochs} | TrainLoss={avg_train_loss:.4f} \"\n",
    "                  f\"| Acc={acc:.4f} | Prec={prec:.4f} | Rec={rec:.4f} | F1={f1:.4f} | AUC={auc:.4f}\")\n",
    "\n",
    "            # check improvement\n",
    "            if f1 > best_val_f1:\n",
    "                best_val_f1 = f1\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
    "                best_preds = val_preds.copy()\n",
    "                best_labels = val_labels_all.copy()\n",
    "                self.best_epoch = epoch\n",
    "                decrease_count = 0\n",
    "                if save_best:\n",
    "                    os.makedirs(best_save_dir, exist_ok=True)\n",
    "                    self.model.save_pretrained(best_save_dir)\n",
    "                    self.tokenizer.save_pretrained(best_save_dir)\n",
    "                    saved_path = best_save_dir\n",
    "                    print(f\"‚úÖ Saved best model (epoch {epoch}) -> {best_save_dir}\")\n",
    "            elif f1 < best_val_f1:\n",
    "                decrease_count += 1\n",
    "                print(f\"‚ö†Ô∏è F1 gi·∫£m so v·ªõi best tr∆∞·ªõc ƒë√≥. decrease_count={decrease_count}/2\")\n",
    "\n",
    "            # early stopping n·∫øu gi·∫£m 2 l·∫ßn li√™n ti·∫øp\n",
    "            if decrease_count >= 2:\n",
    "                print(f\"‚èπÔ∏è Early stopping t·∫°i epoch {epoch} (F1 gi·∫£m 2 l·∫ßn li√™n ti·∫øp).\")\n",
    "                break\n",
    "\n",
    "        # restore best state\n",
    "        if best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "            self.model.to(device)\n",
    "\n",
    "        # confusion matrix\n",
    "        if best_preds is not None and best_labels is not None:\n",
    "            cm = confusion_matrix(best_labels, best_preds)\n",
    "            plt.figure(figsize=(5,4))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                        xticklabels=[\"Real\",\"Fake\"], yticklabels=[\"Real\",\"Fake\"])\n",
    "            plt.title(f\"Best Confusion Matrix ({self.model_name}) - epoch {self.best_epoch}\")\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"True\")\n",
    "            plt.show()\n",
    "\n",
    "        return {\"best_f1\": best_val_f1, \"best_epoch\": self.best_epoch, \"history\": self.history, \"saved_path\": saved_path}\n",
    "\n",
    "    def save_model(self, save_dir: str = \"./saved_model\"):\n",
    "        \"\"\"Save current model/tokenizer (HuggingFace format).\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(save_dir)\n",
    "        self.tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"‚úÖ {self.model_name} saved to {save_dir}\")\n",
    "\n",
    "    def predict(self, texts, batch_size: int = 32):\n",
    "        \"\"\"Batched inference. Returns (preds_array, probs_array).\"\"\"\n",
    "        single_input = False\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            single_input = True\n",
    "\n",
    "        self.model.eval()\n",
    "        ds = self.encode(texts)\n",
    "        loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        all_preds, all_probs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_preds.extend(preds.tolist())\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        if single_input:\n",
    "            return all_preds[0], all_probs[0]\n",
    "        return all_preds, all_probs\n",
    "\n",
    "print(\"‚úÖ UniversalTransformerClassifier defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:21:43.492810Z",
     "iopub.status.busy": "2024-11-02T07:21:43.492199Z",
     "iopub.status.idle": "2024-11-02T07:21:43.664903Z",
     "shell.execute_reply": "2024-11-02T07:21:43.664107Z",
     "shell.execute_reply.started": "2024-11-02T07:21:43.492773Z"
    },
    "executionInfo": {
     "elapsed": 14569,
     "status": "aborted",
     "timestamp": 1757222032185,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "6-g5JYK7YSiT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üîß CELL 6 (MODIFIED): PHOBERT FEATURE EXTRACTOR (support from_scratch)\n",
    "# ===============================\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "class PhoBERTFeatureExtractor:\n",
    "    def __init__(self, model_name='vinai/phobert-base', layer=-2, from_scratch=False):\n",
    "        self.model_name = model_name\n",
    "        self.layer = layer\n",
    "        self.from_scratch = from_scratch\n",
    "\n",
    "        print(f\"üîß Initializing PhoBERTFeatureExtractor (from_scratch={from_scratch}) ...\")\n",
    "        # Keep tokenizer from pretrained vocab\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if from_scratch:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            # Use AutoModel (encoder only) from config -> random init\n",
    "            self.model = AutoModel.from_config(config)\n",
    "            print(\"‚ö†Ô∏è PhoBERT model initialized with RANDOM weights for feature extraction.\")\n",
    "        else:\n",
    "            # Use pretrained weights (original behavior)\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "            print(\"‚úÖ PhoBERT pretrained model loaded for feature extraction.\")\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print(f\"‚úÖ PhoBERT feature extractor ready on {device}\")\n",
    "\n",
    "    def extract_features(self, texts, batch_size=16):\n",
    "        all_features = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting PhoBERT features\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=256\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states[self.layer]\n",
    "                pooled_features = torch.mean(hidden_states, dim=1)\n",
    "                all_features.extend(pooled_features.cpu().numpy())\n",
    "        return np.array(all_features)\n",
    "\n",
    "print(\"‚úÖ PhoBERT Feature Extractor (with from_scratch option) defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:21:47.384821Z",
     "iopub.status.busy": "2024-11-02T07:21:47.384468Z",
     "iopub.status.idle": "2024-11-02T07:21:47.432656Z",
     "shell.execute_reply": "2024-11-02T07:21:47.431925Z",
     "shell.execute_reply.started": "2024-11-02T07:21:47.384790Z"
    },
    "executionInfo": {
     "elapsed": 14568,
     "status": "aborted",
     "timestamp": 1757222032186,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "Yu4Fo31_YSiU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üéØ CELL 7 (MODIFIED): HYBRID MODELS (support passing from_scratch to PhoBERT extractor)\n",
    "# ===============================\n",
    "class HybridPhoBERTClassifier:\n",
    "    def __init__(self, method='tfidf', tfidf_features=5000, w2v_size=200, phobert_from_scratch=False):\n",
    "        self.method = method\n",
    "        # Pass the from_scratch flag to feature extractor\n",
    "        self.phobert_extractor = PhoBERTFeatureExtractor(from_scratch=phobert_from_scratch)\n",
    "\n",
    "        if method == 'tfidf':\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=tfidf_features,\n",
    "                ngram_range=(1, 3),\n",
    "                lowercase=True,\n",
    "                stop_words=None\n",
    "            )\n",
    "        elif method == 'word2vec':\n",
    "            self.w2v_size = w2v_size\n",
    "            self.word2vec_model = None\n",
    "\n",
    "        self.classifier = LogisticRegression(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            C=1.0\n",
    "        )\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        print(f\"üéØ Initialized Hybrid PhoBERT + {method.upper()} classifier (phobert_from_scratch={phobert_from_scratch})\")\n",
    "\n",
    "    # prepare_word2vec_features, train, predict - keep the same as your original implementation\n",
    "    def prepare_word2vec_features(self, texts):\n",
    "        print(\"üî§ Preparing Word2Vec features...\")\n",
    "        tokenized_texts = []\n",
    "        preprocessor = AdvancedVietnameseTextPreprocessor()\n",
    "        for text in tqdm(texts, desc=\"Tokenizing for Word2Vec\"):\n",
    "            tokens = preprocessor.tokenize_vietnamese(text)\n",
    "            if tokens:\n",
    "                tokenized_texts.append(tokens)\n",
    "        if self.word2vec_model is None:\n",
    "            print(\"üî§ Training Word2Vec model...\")\n",
    "            self.word2vec_model = Word2Vec(\n",
    "                sentences=tokenized_texts,\n",
    "                vector_size=self.w2v_size,\n",
    "                window=5,\n",
    "                min_count=2,\n",
    "                workers=4,\n",
    "                epochs=10,\n",
    "                sg=1\n",
    "            )\n",
    "        doc_vectors = []\n",
    "        for tokens in tokenized_texts:\n",
    "            vectors = []\n",
    "            for token in tokens:\n",
    "                if token in self.word2vec_model.wv:\n",
    "                    vectors.append(self.word2vec_model.wv[token])\n",
    "            if vectors:\n",
    "                doc_vector = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                doc_vector = np.zeros(self.w2v_size)\n",
    "            doc_vectors.append(doc_vector)\n",
    "        return np.array(doc_vectors)\n",
    "\n",
    "    def train(self, train_texts, train_labels):\n",
    "        print(f\"üîß Training PhoBERT + {self.method.upper()}...\")\n",
    "        phobert_features = self.phobert_extractor.extract_features(train_texts)\n",
    "        if self.method == 'tfidf':\n",
    "            text_features = self.tfidf_vectorizer.fit_transform(train_texts).toarray()\n",
    "        elif self.method == 'word2vec':\n",
    "            text_features = self.prepare_word2vec_features(train_texts)\n",
    "        combined_features = np.hstack([phobert_features, text_features])\n",
    "        combined_features_scaled = self.feature_scaler.fit_transform(combined_features)\n",
    "        self.classifier.fit(combined_features_scaled, train_labels)\n",
    "        print(f\"‚úÖ PhoBERT + {self.method.upper()} training completed!\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, texts):\n",
    "        phobert_features = self.phobert_extractor.extract_features(texts)\n",
    "        if self.method == 'tfidf':\n",
    "            text_features = self.tfidf_vectorizer.transform(texts).toarray()\n",
    "        elif self.method == 'word2vec':\n",
    "            text_features = self.prepare_word2vec_features(texts)\n",
    "        combined_features = np.hstack([phobert_features, text_features])\n",
    "        combined_features_scaled = self.feature_scaler.transform(combined_features)\n",
    "        predictions = self.classifier.predict(combined_features_scaled)\n",
    "        probabilities = self.classifier.predict_proba(combined_features_scaled)\n",
    "        return predictions, probabilities\n",
    "\n",
    "print(\"‚úÖ Hybrid PhoBERT Classifiers (with phobert_from_scratch option) defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:21:54.202435Z",
     "iopub.status.busy": "2024-11-02T07:21:54.201672Z",
     "iopub.status.idle": "2024-11-02T07:33:45.389705Z",
     "shell.execute_reply": "2024-11-02T07:33:45.388797Z",
     "shell.execute_reply.started": "2024-11-02T07:21:54.202394Z"
    },
    "executionInfo": {
     "elapsed": 14567,
     "status": "aborted",
     "timestamp": 1757222032186,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "8O7L0AwuYSiW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üìä CELL 8: COMPREHENSIVE MODEL EVALUATOR\n",
    "# ===============================\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "\n",
    "class ComprehensiveModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.predictions = {}\n",
    "        self.probabilities = {}\n",
    "\n",
    "    def evaluate_model(self, y_true, y_pred, y_prob=None, model_name=\"Model\"):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "\n",
    "        # Basic metrics\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        }\n",
    "\n",
    "        # AUC if probabilities are provided\n",
    "        if y_prob is not None and y_prob.shape[1] == 2:\n",
    "            try:\n",
    "                results['auc'] = roc_auc_score(y_true, y_prob[:, 1])\n",
    "            except:\n",
    "                results['auc'] = 0.0\n",
    "\n",
    "        self.results[model_name] = results\n",
    "        self.predictions[model_name] = y_pred\n",
    "        if y_prob is not None:\n",
    "            self.probabilities[model_name] = y_prob\n",
    "\n",
    "        print(f\"\\nüìä {model_name} Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric, value in results.items():\n",
    "            print(f\"{metric.upper():15}: {value:.4f}\")\n",
    "\n",
    "        # Detailed classification report\n",
    "        print(f\"\\nüìã Classification Report - {model_name}:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=['Real', 'Fake'], digits=4))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_confusion_matrices(self, y_true, models_to_plot=None):\n",
    "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "        if models_to_plot is None:\n",
    "            models_to_plot = list(self.predictions.keys())\n",
    "\n",
    "        n_models = len(models_to_plot)\n",
    "        if n_models == 0:\n",
    "            return\n",
    "\n",
    "        # Calculate grid dimensions\n",
    "        n_cols = min(3, n_models)\n",
    "        n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for idx, model_name in enumerate(models_to_plot):\n",
    "            row = idx // n_cols\n",
    "            col = idx % n_cols\n",
    "            ax = axes[row, col] if n_rows > 1 else axes[col]\n",
    "\n",
    "            cm = confusion_matrix(y_true, self.predictions[model_name])\n",
    "\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                       xticklabels=['Real', 'Fake'],\n",
    "                       yticklabels=['Real', 'Fake'])\n",
    "            ax.set_title(f'{model_name}')\n",
    "            ax.set_ylabel('True Label')\n",
    "            ax.set_xlabel('Predicted Label')\n",
    "\n",
    "        # Hide empty subplots\n",
    "        for idx in range(n_models, n_rows * n_cols):\n",
    "            if n_rows > 1:\n",
    "                row = idx // n_cols\n",
    "                col = idx % n_cols\n",
    "                fig.delaxes(axes[row, col])\n",
    "            elif n_cols > 1:\n",
    "                fig.delaxes(axes[idx])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curves(self, y_true):\n",
    "        \"\"\"Plot ROC curves for models with probabilities\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        for model_name, y_prob in self.probabilities.items():\n",
    "            if y_prob.shape[1] == 2:  # Binary classification\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "                auc_score = roc_auc_score(y_true, y_prob[:, 1])\n",
    "                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves Comparison')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def compare_models(self, plot_confusion=True, plot_roc=True):\n",
    "        \"\"\"Comprehensive model comparison\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to compare!\")\n",
    "            return None\n",
    "\n",
    "        df_results = pd.DataFrame(self.results).T\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üèÜ COMPREHENSIVE MODEL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(df_results.round(4))\n",
    "\n",
    "        # Plot metrics comparison\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "        # Accuracy, Precision, Recall, F1\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_weighted']\n",
    "        colors = ['skyblue', 'lightgreen', 'salmon', 'gold']\n",
    "\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[i//2, i%2]\n",
    "            bars = ax.bar(df_results.index, df_results[metric], color=colors[i])\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_ylim(0, 1)\n",
    "\n",
    "            # Add value labels on bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot confusion matrices if requested\n",
    "        if plot_confusion and self.predictions:\n",
    "            y_true_sample = None\n",
    "            for pred in self.predictions.values():\n",
    "                y_true_sample = pred  # We'll need the actual y_true from outside\n",
    "                break\n",
    "            # Note: This will be called with actual y_true from the main pipeline\n",
    "\n",
    "        return df_results\n",
    "\n",
    "    def get_best_model(self, metric='f1_weighted'):\n",
    "        \"\"\"Get the best performing model\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "\n",
    "        best_score = -1\n",
    "        best_model = None\n",
    "\n",
    "        for model_name, results in self.results.items():\n",
    "            if results.get(metric, 0) > best_score:\n",
    "                best_score = results[metric]\n",
    "                best_model = model_name\n",
    "\n",
    "        print(f\"üèÖ Best model: {best_model} with {metric} = {best_score:.4f}\")\n",
    "        return best_model, best_score\n",
    "\n",
    "print(\"‚úÖ Comprehensive Model Evaluator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:37:35.792674Z",
     "iopub.status.busy": "2024-11-02T07:37:35.792315Z",
     "iopub.status.idle": "2024-11-02T07:37:35.797727Z",
     "shell.execute_reply": "2024-11-02T07:37:35.796797Z",
     "shell.execute_reply.started": "2024-11-02T07:37:35.792642Z"
    },
    "executionInfo": {
     "elapsed": 14565,
     "status": "aborted",
     "timestamp": 1757222032187,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "_Xz9Row7YSiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üîÑ CELL 9: ENHANCED SINGLE-MODEL PIPELINE CLASS (FIXED)\n",
    "# ===============================\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import itertools\n",
    "\n",
    "class SingleModelPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline c·∫£i ti·∫øn cho training model v·ªõi early stopping v√† detailed metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, preprocessor=None, evaluator=None):\n",
    "        self.preprocessor = preprocessor or EnhancedNewsDatasetPreprocessor()\n",
    "        self.evaluator = evaluator or ComprehensiveModelEvaluator()\n",
    "        self.model = None\n",
    "        self.model_name = None\n",
    "        self.full_data = None\n",
    "        self.training_history = []\n",
    "\n",
    "    def load_data(self, fake_path, real_path, usecols=None):\n",
    "        print(\"üìÇ Loading datasets...\")\n",
    "        if usecols is None:\n",
    "            usecols = ['domain','title','publish_date','summary','content_html','label']\n",
    "        try:\n",
    "            df_fake = pd.read_csv(fake_path, usecols=usecols)\n",
    "            df_real = pd.read_csv(real_path, usecols=usecols)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading data: {e}\")\n",
    "\n",
    "        print(f\"üìä Fake: {len(df_fake)} | Real: {len(df_real)}\")\n",
    "        df = pd.concat([df_fake, df_real], axis=0).reset_index(drop=True)\n",
    "        print(f\"üìä Combined dataset: {len(df)}\")\n",
    "        print(\"üìä Label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "        return df\n",
    "\n",
    "    def prepare_and_split(self, df, test_size=0.2, balance_data=False, sample_size=None, random_state=42):\n",
    "        df_clean = self.preprocessor.prepare_data(df, balance_data=balance_data)\n",
    "        if sample_size is not None:\n",
    "            df_clean = df_clean.sample(n=min(sample_size, len(df_clean)), random_state=random_state)\n",
    "        self.full_data = df_clean.copy()\n",
    "        X = df_clean['combined_text'].tolist()\n",
    "        y = df_clean['label'].tolist()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "        print(f\"üìä Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _init_model(self, model_key, **kwargs):\n",
    "        model_key = model_key.lower()\n",
    "        self.model_name = model_key\n",
    "        if model_key == 'phobert':\n",
    "            print(\"‚öôÔ∏è Init PhoBERT (transformer)\")\n",
    "            model = UniversalTransformerClassifier('vinai/phobert-base', max_length=256, from_scratch=kwargs.get('from_scratch', True))\n",
    "        elif model_key == 'bert':\n",
    "            print(\"‚öôÔ∏è Init BERT (multilingual)\")\n",
    "            model = UniversalTransformerClassifier('bert-base-multilingual-cased', max_length=512, from_scratch=kwargs.get('from_scratch', True))\n",
    "        elif model_key == 'roberta':\n",
    "            print(\"‚öôÔ∏è Init RoBERTa\")\n",
    "            model = UniversalTransformerClassifier('roberta-base', max_length=512, from_scratch=kwargs.get('from_scratch', True))\n",
    "        elif model_key == 'phobert_tfidf':\n",
    "            print(\"‚öôÔ∏è Init PhoBERT + TF-IDF (hybrid)\")\n",
    "            model = HybridPhoBERTClassifier(method='tfidf', tfidf_features=kwargs.get('tfidf_features', 10000), phobert_from_scratch=kwargs.get('from_scratch', True))\n",
    "        elif model_key == 'phobert_w2v':\n",
    "            print(\"‚öôÔ∏è Init PhoBERT + Word2Vec (hybrid)\")\n",
    "            model = HybridPhoBERTClassifier(method='word2vec', w2v_size=kwargs.get('w2v_size', 300), phobert_from_scratch=kwargs.get('from_scratch', True))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_key: {model_key}\")\n",
    "        return model\n",
    "\n",
    "    def train_single_model(self, model_key, X_train, y_train, X_val, y_val,\n",
    "                           num_epochs=15, batch_size=16, save_best=True,\n",
    "                           patience=2, model_save_dir='./saved_models',\n",
    "                           extra_init_kwargs=None):\n",
    "        \"\"\"\n",
    "        Enhanced training v·ªõi detailed metrics tracking v√† early stopping\n",
    "        \"\"\"\n",
    "        extra_init_kwargs = extra_init_kwargs or {}\n",
    "        self.model = self._init_model(model_key, **extra_init_kwargs)\n",
    "\n",
    "        # Initialize tracking variables\n",
    "        best_f1 = -1.0\n",
    "        best_epoch = -1\n",
    "        bad_epochs = 0\n",
    "        self.training_history = []\n",
    "\n",
    "        print(f\"\\nüöÄ Starting training: {model_key.upper()}\")\n",
    "        print(f\"üìä Epochs: {num_epochs} | Batch size: {batch_size} | Patience: {patience}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # For transformer models, use the built-in train method\n",
    "        if hasattr(self.model, 'train') and model_key in ['phobert', 'bert', 'roberta']:\n",
    "            results = self.model.train(\n",
    "                X_train, y_train, X_val, y_val,\n",
    "                num_epochs=num_epochs,\n",
    "                batch_size=batch_size,\n",
    "                save_best=save_best,\n",
    "                best_save_dir=os.path.join(model_save_dir, model_key)\n",
    "            )\n",
    "\n",
    "            # Extract final predictions for evaluation\n",
    "            final_preds, final_probs = self.model.predict(X_val, batch_size=batch_size)\n",
    "\n",
    "            # Calculate final metrics\n",
    "            final_metrics = self._calculate_metrics(y_val, final_preds, final_probs)\n",
    "\n",
    "            return {\n",
    "                'history': results.get('history', {}),\n",
    "                'best_f1': results.get('best_f1', final_metrics['f1_weighted']),\n",
    "                'best_epoch': results.get('best_epoch', num_epochs),\n",
    "                'saved_path': results.get('saved_path'),\n",
    "                'preds': final_preds,\n",
    "                'probs': final_probs,\n",
    "                'metrics': final_metrics\n",
    "            }\n",
    "\n",
    "        # For hybrid models, implement custom training loop\n",
    "        elif model_key in ['phobert_tfidf', 'phobert_w2v']:\n",
    "            print(f\"üîß Training {model_key.upper()} hybrid model...\")\n",
    "\n",
    "            # Train the hybrid model (no epoch-by-epoch for hybrid models)\n",
    "            self.model.train(X_train, y_train)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_preds, val_probs = self.model.predict(X_val)\n",
    "            metrics = self._calculate_metrics(y_val, val_preds, val_probs)\n",
    "\n",
    "            # Save model\n",
    "            saved_path = None\n",
    "            if save_best:\n",
    "                saved_path = self._save_hybrid_model(model_key, model_save_dir)\n",
    "\n",
    "            return {\n",
    "                'history': [{'epoch': 1, **metrics}],\n",
    "                'best_f1': metrics['f1_weighted'],\n",
    "                'best_epoch': 1,\n",
    "                'saved_path': saved_path,\n",
    "                'preds': val_preds,\n",
    "                'probs': val_probs,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "\n",
    "    def _calculate_metrics(self, y_true, y_pred, y_prob=None):\n",
    "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        }\n",
    "\n",
    "        if y_prob is not None and len(y_prob.shape) > 1 and y_prob.shape[1] == 2:\n",
    "            try:\n",
    "                metrics['auc'] = roc_auc_score(y_true, y_prob[:, 1])\n",
    "            except:\n",
    "                metrics['auc'] = 0.0\n",
    "        else:\n",
    "            metrics['auc'] = 0.0\n",
    "\n",
    "        # Classification report\n",
    "        metrics['classification_report'] = classification_report(\n",
    "            y_true, y_pred, target_names=['Real', 'Fake'], digits=4\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _save_hybrid_model(self, model_key, model_save_dir):\n",
    "        \"\"\"Save hybrid model using joblib\"\"\"\n",
    "        try:\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "            model_path = os.path.join(model_save_dir, f\"{model_key}.joblib\")\n",
    "            joblib.dump(self.model, model_path)\n",
    "            print(f\"‚úÖ Hybrid model saved to {model_path}\")\n",
    "            return model_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving hybrid model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels=None, title=None):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        if labels is None:\n",
    "            labels = [0, 1]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Real', 'Fake'],\n",
    "                   yticklabels=['Real', 'Fake'])\n",
    "\n",
    "        if title is None:\n",
    "            title = f'Confusion Matrix - {self.model_name.upper()}'\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return cm\n",
    "\n",
    "    def plot_history(self, history):\n",
    "        \"\"\"Plot training history if available\"\"\"\n",
    "        if not history or not isinstance(history, dict):\n",
    "            print(\"‚ö†Ô∏è No training history available to plot\")\n",
    "            return\n",
    "\n",
    "        metrics_to_plot = []\n",
    "        if 'val_f1' in history:\n",
    "            metrics_to_plot.extend(['val_f1', 'val_acc', 'val_prec', 'val_rec'])\n",
    "        elif 'f1_weighted' in history[0] if isinstance(history, list) else False:\n",
    "            # For list format history\n",
    "            epochs = [h['epoch'] for h in history]\n",
    "            f1_scores = [h['f1_weighted'] for h in history]\n",
    "            accuracies = [h['accuracy'] for h in history]\n",
    "\n",
    "            plt.figure(figsize=(12, 4))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, f1_scores, 'bo-', label='F1 Score')\n",
    "            plt.title('F1 Score over Epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, accuracies, 'ro-', label='Accuracy')\n",
    "            plt.title('Accuracy over Epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return\n",
    "\n",
    "        if not metrics_to_plot:\n",
    "            print(\"‚ö†Ô∏è No plottable metrics found in history\")\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'Training History - {self.model_name.upper()}', fontsize=16)\n",
    "\n",
    "        epochs = range(1, len(history[metrics_to_plot[0]]) + 1)\n",
    "\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            if metric in history:\n",
    "                ax = axes[i//2, i%2]\n",
    "                ax.plot(epochs, history[metric], 'o-', linewidth=2)\n",
    "                ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Score')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_ylim(0, 1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14563,
     "status": "aborted",
     "timestamp": 1757222032187,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "aSU59Fg__z2N"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üîß CELL 10: PhoBERT + TF-IDF Concatenate Classifier\n",
    "# ===============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class HybridPhoBERTTFIDFConcatClassifier:\n",
    "    def __init__(self, tfidf_features=10000, phobert_from_scratch=False,\n",
    "                 hidden_sizes=(256,128), dropout=0.3, lr=1e-4,\n",
    "                 epochs=15, batch_size=32, device=None):\n",
    "        # TF-IDF setup\n",
    "        self.vectorizer = TfidfVectorizer(max_features=tfidf_features)\n",
    "        self.scaler = StandardScaler(with_mean=False)  # gi·ªØ sparse format\n",
    "\n",
    "        # PhoBERT setup\n",
    "        from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "        model_name = \"vinai/phobert-base\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if phobert_from_scratch:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            self.phobert = AutoModel.from_config(config)\n",
    "        else:\n",
    "            self.phobert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Device\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.phobert.to(self.device)\n",
    "        self.phobert.eval()  # freeze PhoBERT (feature extractor)\n",
    "\n",
    "        # Classifier (MLP)\n",
    "        input_size = 768 + tfidf_features  # PhoBERT hidden_size + TF-IDF\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = h\n",
    "        layers.append(nn.Linear(prev_size, 2))  # binary classification\n",
    "        self.classifier = nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "        # Optimizer & loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "\n",
    "        # Training params\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _extract_phobert_features(self, texts):\n",
    "        \"\"\"Extract CLS embeddings from PhoBERT\"\"\"\n",
    "        all_features = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), self.batch_size):\n",
    "                batch = texts[i:i+self.batch_size]\n",
    "                inputs = self.tokenizer(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(self.device)\n",
    "                outputs = self.phobert(**inputs)\n",
    "                cls_embeddings = outputs.last_hidden_state[:,0,:]  # CLS token\n",
    "                all_features.append(cls_embeddings.cpu())\n",
    "        return torch.cat(all_features, dim=0).numpy()\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print(\"üîß Training PhoBERT + TF-IDF Concatenate model...\")\n",
    "\n",
    "        # TF-IDF fit + transform\n",
    "        X_tfidf = self.vectorizer.fit_transform(X_train).toarray()\n",
    "        X_tfidf = self.scaler.fit_transform(X_tfidf)\n",
    "\n",
    "        # PhoBERT features\n",
    "        X_phobert = self._extract_phobert_features(X_train)\n",
    "\n",
    "        # Concatenate\n",
    "        X_combined = np.concatenate([X_phobert, X_tfidf], axis=1)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(X_combined, dtype=torch.float32), y_train)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.classifier.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.classifier(xb)\n",
    "                loss = self.criterion(out, yb)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"üìâ Epoch {epoch+1}/{self.epochs} - Loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TF-IDF transform\n",
    "        X_tfidf = self.vectorizer.transform(X).toarray()\n",
    "        X_tfidf = self.scaler.transform(X_tfidf)\n",
    "\n",
    "        # PhoBERT features\n",
    "        X_phobert = self._extract_phobert_features(X)\n",
    "\n",
    "        # Concatenate\n",
    "        X_combined = np.concatenate([X_phobert, X_tfidf], axis=1)\n",
    "        X_tensor = torch.tensor(X_combined, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(X_tensor)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "        return preds, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:37:41.052338Z",
     "iopub.status.busy": "2024-11-02T07:37:41.051938Z",
     "iopub.status.idle": "2024-11-02T07:37:41.477222Z",
     "shell.execute_reply": "2024-11-02T07:37:41.476235Z",
     "shell.execute_reply.started": "2024-11-02T07:37:41.052300Z"
    },
    "executionInfo": {
     "elapsed": 14564,
     "status": "aborted",
     "timestamp": 1757222032190,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "k_63KncNYSia",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üéØ CELL 11: PREDICTION & SAVE UTILITIES\n",
    "# ===============================\n",
    "def predict_single_text(pipeline: SingleModelPipeline, text, label_map=None):\n",
    "    \"\"\"D·ª± ƒëo√°n 1 vƒÉn b·∫£n v·ªõi model hi·ªán t·∫°i trong pipeline.\"\"\"\n",
    "    if pipeline.model is None:\n",
    "        print(\"‚ùå Ch∆∞a c√≥ model trong pipeline. Train ho·∫∑c load model tr∆∞·ªõc.\")\n",
    "        return None\n",
    "    try:\n",
    "        pred, prob = pipeline.model.predict([text])\n",
    "    except TypeError:\n",
    "        pred, prob = pipeline.model.predict([text])\n",
    "    label_map = label_map or {0: 'REAL', 1: 'FAKE'}\n",
    "    label = label_map.get(pred[0], pred[0])\n",
    "    confidence = np.max(prob[0])\n",
    "    print(f\"\\nüìù Text preview: {text[:200]}...\")\n",
    "    print(f\"üéØ Model: {pipeline.model_name}\")\n",
    "    print(f\"‚úÖ Prediction: {label} (Confidence={confidence:.4f})\")\n",
    "    print(f\"üìä Probabilities: {prob[0]}\")\n",
    "    return pred[0], confidence, prob[0]\n",
    "\n",
    "def save_pipeline_model(pipeline: SingleModelPipeline, base_path=\"./saved_models\"):\n",
    "    \"\"\"L∆∞u model hi·ªán t·∫°i (transformer -> save_pretrained; otherwise joblib).\"\"\"\n",
    "    if pipeline.model is None:\n",
    "        print(\"‚ùå No model to save.\")\n",
    "        return None\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    try:\n",
    "        model_key = pipeline.model_name or \"model\"\n",
    "        if hasattr(pipeline.model, 'model') and hasattr(pipeline.model, 'tokenizer'):\n",
    "            model_dir = os.path.join(base_path, model_key)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            pipeline.model.model.save_pretrained(model_dir)\n",
    "            pipeline.model.tokenizer.save_pretrained(model_dir)\n",
    "            print(f\"‚úÖ Transformer saved to {model_dir}\")\n",
    "            return model_dir\n",
    "        else:\n",
    "            model_file = os.path.join(base_path, f\"{model_key}.joblib\")\n",
    "            joblib.dump(pipeline.model, model_file)\n",
    "            print(f\"‚úÖ Model saved to {model_file}\")\n",
    "            return model_file\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving model: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T07:37:45.734940Z",
     "iopub.status.busy": "2024-11-02T07:37:45.734564Z",
     "iopub.status.idle": "2024-11-02T07:37:45.744475Z",
     "shell.execute_reply": "2024-11-02T07:37:45.743554Z",
     "shell.execute_reply.started": "2024-11-02T07:37:45.734905Z"
    },
    "executionInfo": {
     "elapsed": 14563,
     "status": "aborted",
     "timestamp": 1757222032191,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "uk1zi6ByYSic",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üß™ CELL 12: ENHANCED MAIN EXECUTION (UPDATED + METRICS & PLOTS)\n",
    "# ===============================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- C·∫•u h√¨nh ---\n",
    "MODEL_NAME = \"phobert_tfidf_concat\"   # <-- Th√™m t√πy ch·ªçn m·ªõi\n",
    "FAKE_PATH = \"/content/drive/MyDrive/Fake_New_Detection_Project-20250906T205115Z-1-001/Fake_New_Detection_Project/NLP Project - Thay Khanh/ViFN-Vietnamese_Fake_New_Datasets_Ver3-main/processed/deduplicated_articles_fake.csv\"\n",
    "REAL_PATH = \"/content/drive/MyDrive/Fake_New_Detection_Project-20250906T205115Z-1-001/Fake_New_Detection_Project/NLP Project - Thay Khanh/ViFN-Vietnamese_Fake_New_Datasets_Ver3-main/processed/deduplicated_articles_real.csv\"\n",
    "TEST_SIZE = 0.2\n",
    "SAMPLE_SIZE = None\n",
    "BALANCE_DATA = False\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 2\n",
    "MODEL_SAVE_DIR = \"/content/drive/MyDrive/Fake_New_Detection_Project-20250906T205115Z-1-001/Fake_New_Detection_Project/NLP Project - Thay Khanh/ViFN-Vietnamese_Fake_New_Datasets_Ver3-main/PhoBERT+TF-IDF\"\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ VIETNAMESE FAKE NEWS DETECTION - ENHANCED TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Initialize pipeline\n",
    "    pipeline = SingleModelPipeline()\n",
    "\n",
    "    # Load and prepare data\n",
    "    print(\"\\nüìÇ STEP 1: Data Loading and Preparation\")\n",
    "    df = pipeline.load_data(FAKE_PATH, REAL_PATH)\n",
    "    X_train, X_test, y_train, y_test = pipeline.prepare_and_split(\n",
    "        df,\n",
    "        test_size=TEST_SIZE,\n",
    "        balance_data=BALANCE_DATA,\n",
    "        sample_size=SAMPLE_SIZE\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    print(f\"\\nü§ñ STEP 2: Training Model - {MODEL_NAME.upper()}\")\n",
    "    if MODEL_NAME == \"phobert_tfidf_concat\":\n",
    "        model = HybridPhoBERTTFIDFConcatClassifier(\n",
    "            tfidf_features=10000,\n",
    "            hidden_sizes=(256,128),\n",
    "            dropout=0.3,\n",
    "            lr=1e-4,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        # Train v√† l·∫•y l·ªãch s·ª≠ metrics\n",
    "        preds, probs, history = model.train(X_train, y_train, X_test, y_test, return_history=True)\n",
    "\n",
    "        # Evaluate tr√™n test set cu·ªëi c√πng\n",
    "        metrics, best_f1 = pipeline.evaluate_model(y_test, preds, probs)\n",
    "        results = {\n",
    "            'best_f1': best_f1,\n",
    "            'best_epoch': max(history[\"epoch\"], key=lambda i: history[\"f1\"][i-1]),\n",
    "            'metrics': metrics,\n",
    "            'preds': preds,\n",
    "            'saved_path': None,\n",
    "            'history': history\n",
    "        }\n",
    "    else:\n",
    "        # Gi·ªØ nguy√™n cho c√°c model kh√°c (bert, roberta, phobert,‚Ä¶)\n",
    "        results = pipeline.train_single_model(\n",
    "            MODEL_NAME,\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            patience=PATIENCE,\n",
    "            save_best=True,\n",
    "            model_save_dir=MODEL_SAVE_DIR,\n",
    "            extra_init_kwargs={'from_scratch': True}\n",
    "        )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nüìä STEP 3: Results Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üèÖ Best F1 Score: {results['best_f1']:.4f}\")\n",
    "    print(f\"üèÜ Best Epoch: {results['best_epoch']}\")\n",
    "    print(f\"üíæ Model Saved: {results.get('saved_path', 'Not saved')}\")\n",
    "\n",
    "    # Show detailed metrics\n",
    "    print(f\"\\nüìã DETAILED METRICS - {MODEL_NAME.upper()}:\")\n",
    "    print(\"-\" * 60)\n",
    "    metrics = results['metrics']\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'classification_report':\n",
    "            print(f\"{metric.upper():15}: {value:.4f}\")\n",
    "\n",
    "    # Show classification report\n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    print(f\"\\nüìä STEP 4: Visualization\")\n",
    "    pipeline.plot_confusion_matrix(\n",
    "        y_test,\n",
    "        results['preds'],\n",
    "        title=f'Confusion Matrix - {MODEL_NAME.upper()} (F1={results[\"best_f1\"]:.4f})'\n",
    "    )\n",
    "\n",
    "    # Plot training history (loss + acc + prec + rec + f1 + auc)\n",
    "    if results.get('history') is not None:\n",
    "        pipeline.plot_history(results['history'])\n",
    "\n",
    "    print(f\"\\n‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"üéØ Final Model: {MODEL_NAME.upper()}\")\n",
    "    print(f\"üìà Best Performance: F1={results['best_f1']:.4f} at Epoch {results['best_epoch']}\")\n",
    "\n",
    "    return pipeline, results\n",
    "\n",
    "# Run the main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline, results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14562,
     "status": "aborted",
     "timestamp": 1757222032192,
     "user": {
      "displayName": "Kiet Lam",
      "userId": "04586900742867251748"
     },
     "user_tz": -420
    },
    "id": "hjU2F7wu91YM"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üß™ CELL 13: ENHANCED TEST PREDICTIONS\n",
    "# ===============================\n",
    "def enhanced_test_predictions(pipeline):\n",
    "    \"\"\"Test the trained model with sample texts\"\"\"\n",
    "    test_texts = [\n",
    "        \"Ch√≠nh ph·ªß Vi·ªát Nam c√¥ng b·ªë ch√≠nh s√°ch m·ªõi v·ªÅ ph√°t tri·ªÉn kinh t·∫ø s·ªë trong nƒÉm 2024\",\n",
    "        \"N√ìNG: Ph√°t hi·ªán lo·∫°i th·ª±c ph·∫©m c√≥ th·ªÉ ch·ªØa kh·ªèi m·ªçi b·ªánh ung th∆∞ ch·ªâ trong 3 ng√†y!\",\n",
    "        \"Nghi√™n c·ª©u khoa h·ªçc m·ªõi cho th·∫•y t√°c ƒë·ªông t√≠ch c·ª±c c·ªßa vi·ªác t·∫≠p th·ªÉ d·ª•c ƒë·ªëi v·ªõi s·ª©c kh·ªèe\",\n",
    "        \"SHOCK: Tr√°i ƒë·∫•t s·∫Ω k·∫øt th√∫c v√†o nƒÉm 2025 theo l·ªùi ti√™n tri c·ªï ƒë·∫°i!\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ TESTING PREDICTIONS - {pipeline.model_name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_data = []\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        try:\n",
    "            pred, prob = pipeline.model.predict([text])\n",
    "            if isinstance(pred, (list, np.ndarray)):\n",
    "                pred = pred[0]\n",
    "            if isinstance(prob, (list, np.ndarray)) and len(prob) > 0:\n",
    "                if isinstance(prob[0], (list, np.ndarray)):\n",
    "                    prob = prob[0]\n",
    "                else:\n",
    "                    prob = prob\n",
    "\n",
    "            label = 'FAKE' if pred == 1 else 'REAL'\n",
    "            confidence = np.max(prob) if hasattr(prob, '__len__') else prob\n",
    "\n",
    "            print(f\"\\nüìù Test {i}: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "            print(f\"üéØ Prediction: {label}\")\n",
    "            print(f\"üìä Confidence: {confidence:.4f}\")\n",
    "            print(f\"üìà Probabilities: Real={prob[0]:.4f}, Fake={prob[1]:.4f}\")\n",
    "\n",
    "            results_data.append({\n",
    "                'Text_Preview': text[:120] + ('...' if len(text) > 120 else ''),\n",
    "                'Prediction': label,\n",
    "                'Confidence': f\"{confidence:.4f}\",\n",
    "                'Real_Prob': f\"{prob[0]:.4f}\",\n",
    "                'Fake_Prob': f\"{prob[1]:.4f}\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error predicting text {i}: {e}\")\n",
    "            results_data.append({\n",
    "                'Text_Preview': text[:120] + ('...' if len(text) > 120 else ''),\n",
    "                'Prediction': 'ERROR',\n",
    "                'Confidence': '0.0000',\n",
    "                'Real_Prob': '0.0000',\n",
    "                'Fake_Prob': '0.0000'\n",
    "            })\n",
    "\n",
    "    # Display results table\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    print(f\"\\nüìã PREDICTION RESULTS SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_results)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# Run test predictions\n",
    "test_results = enhanced_test_predictions(pipeline)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5995387,
     "sourceId": 9785358,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
