{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9785358,"sourceType":"datasetVersion","datasetId":5995387}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# ===============================\n","# üì¶ CELL 1: INSTALL PACKAGES (FIXED VERSION)\n","# ===============================\n","# Install core packages first\n","!pip install transformers datasets torch scikit-learn matplotlib seaborn tqdm gensim nltk -q\n","\n","# Install Vietnamese NLP packages with error handling\n","import subprocess\n","import sys\n","\n","def install_with_fallback(package_name, fallback_msg=\"\"):\n","    try:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"-q\"])\n","        print(f\"‚úÖ {package_name} installed successfully\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"‚ö†Ô∏è Failed to install {package_name}: {e}\")\n","        if fallback_msg:\n","            print(f\"   {fallback_msg}\")\n","        return False\n","\n","# Try to install Vietnamese packages\n","print(\"üì¶ Installing Vietnamese NLP packages...\")\n","pyvi_available = install_with_fallback(\"pyvi\", \"Will use basic tokenization instead\")\n","underthesea_available = install_with_fallback(\"underthesea\", \"Will use alternative tokenization\")\n","\n","# Optional packages\n","install_with_fallback(\"wordcloud\", \"Word cloud generation will be skipped\")\n","\n","print(\"\\n‚úÖ Core packages installed successfully!\")\n","print(f\"üìä PyVi available: {pyvi_available}\")\n","print(f\"üìä Underthesea available: {underthesea_available}\")\n","\n","# Alternative minimal installation if Vietnamese packages fail\n","if not pyvi_available and not underthesea_available:\n","    print(\"\\n‚ö†Ô∏è Vietnamese tokenizers not available - using basic preprocessing\")\n","    print(\"This will still work but with reduced Vietnamese text processing quality\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKxJUrWz7Lo0","executionInfo":{"status":"ok","timestamp":1757172339134,"user_tz":-420,"elapsed":33146,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"abd50a23-9f62-44ad-dccb-2c1883399a3e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["üì¶ Installing Vietnamese NLP packages...\n","‚úÖ pyvi installed successfully\n","‚ö†Ô∏è Failed to install underthesea: Command '['/usr/bin/python3', '-m', 'pip', 'install', 'underthesea', '-q']' returned non-zero exit status 2.\n","   Will use alternative tokenization\n","‚úÖ wordcloud installed successfully\n","\n","‚úÖ Core packages installed successfully!\n","üìä PyVi available: True\n","üìä Underthesea available: False\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üìÇ CELL 2: MOUNT DRIVE & IMPORTS\n","# ===============================\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zbA8LWQOOhj","executionInfo":{"status":"ok","timestamp":1757172341928,"user_tz":-420,"elapsed":2799,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"e114a233-7c61-4314-dc62-454a063b08cc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import warnings\n","import re\n","import html\n","import os\n","import pickle\n","import joblib\n","from collections import Counter\n","warnings.filterwarnings('ignore')\n","\n","# Check GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"üî• Using device: {device}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",")\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","from transformers import (\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    Trainer, TrainingArguments, DataCollatorWithPadding,\n","    EarlyStoppingCallback, AutoConfig\n",")\n","from datasets import Dataset\n","\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","nltk.download('punkt', quiet=True)\n","\n","# Vietnamese text processing\n","try:\n","    from pyvi import ViTokenizer\n","    PYVI_AVAILABLE = True\n","except ImportError:\n","    print(\"‚ö†Ô∏è PyVi not available, will use basic tokenization\")\n","    PYVI_AVAILABLE = False\n","\n","try:\n","    from underthesea import word_tokenize\n","    UNDERTHESEA_AVAILABLE = True\n","except ImportError:\n","    print(\"‚ö†Ô∏è Underthesea not available, will use alternative tokenization\")\n","    UNDERTHESEA_AVAILABLE = False\n","\n","print(\"‚úÖ All imports successful!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYB8AhHucQEM","executionInfo":{"status":"ok","timestamp":1757172363349,"user_tz":-420,"elapsed":21416,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"30e52094-ff60-4224-9244-2d47f32325c2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["üî• Using device: cuda\n","GPU: Tesla T4\n","Memory: 14.7 GB\n","‚ö†Ô∏è Underthesea not available, will use alternative tokenization\n","‚úÖ All imports successful!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üîß CELL 3: ADVANCED VIETNAMESE TEXT PREPROCESSOR\n","# ===============================\n","class AdvancedVietnameseTextPreprocessor:\n","    def __init__(self):\n","        self.html_tags = re.compile('<.*?>')\n","\n","        # Vietnamese stopwords (expanded list)\n","        self.vietnamese_stopwords = {\n","            'v√†', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'n√†y', 'ƒë√≥', 'c√°c', 'm·ªôt', 'kh√¥ng', 'ƒë·ªÉ', 'trong',\n","            'c·ªßa', 'v·ªõi', 'v·ªÅ', 't·ª´', 'theo', 'nh∆∞', 'tr√™n', 'd∆∞·ªõi', 'sau', 'tr∆∞·ªõc',\n","            'ƒë√£', 's·∫Ω', 'ƒëang', 'b·ªã', 'cho', 't·∫°i', 'do', 'v√¨', 'n√™n', 'm√†', 'hay',\n","            'ho·∫∑c', 'nh∆∞ng', 'tuy', 'd√π', 'n·∫øu', 'khi', 'l√∫c', 'b√¢y_gi·ªù', 'hi·ªán_t·∫°i',\n","            'ng√†y', 'th√°ng', 'nƒÉm', 'gi·ªù', 'ph√∫t', 'gi√¢y', 'r·ªìi', 'ƒë√¢y', 'kia'\n","        }\n","\n","        # Common replacements for text normalization\n","        self.replacements = {\n","            # Number normalization\n","            r'\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}': ' <DATE> ',  # dates\n","            r'\\d+[.,]\\d+': ' <NUMBER> ',  # decimal numbers\n","            r'\\d+': ' <NUMBER> ',  # integers\n","\n","            # Special characters normalization\n","            r'[!]{2,}': ' <EXCLAMATION> ',\n","            r'[?]{2,}': ' <QUESTION> ',\n","            r'[.]{3,}': ' <DOTS> ',\n","\n","            # Repeated characters\n","            r'(.)\\1{2,}': r'\\1\\1',  # reduce repeated chars to max 2\n","        }\n","\n","        # Common fake news indicators in Vietnamese\n","        self.fake_indicators = {\n","            'n√≥ng', 'hot', 'shock', 'kh·∫©n_c·∫•p', 'c·∫£nh_b√°o', 'nguy_hi·ªÉm',\n","            'b√≠_m·∫≠t', 'ti·∫øt_l·ªô', 'ph√°t_hi·ªán', 'ƒë·ªôt_ph√°', '100%', 'ch·∫Øc_ch·∫Øn',\n","            'tuy·ªát_ƒë·ªëi', 'kh√¥ng_bao_gi·ªù', 'lu√¥n_lu√¥n', 'm√£i_m√£i'\n","        }\n","\n","    def clean_html(self, text):\n","        \"\"\"Advanced HTML cleaning\"\"\"\n","        if pd.isna(text):\n","            return \"\"\n","\n","        # Decode HTML entities\n","        text = html.unescape(str(text))\n","\n","        # Remove HTML tags but keep some structure\n","        text = self.html_tags.sub(' ', text)\n","\n","        # Remove script and style content\n","        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n","        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n","\n","        return text\n","\n","    def normalize_text(self, text):\n","        \"\"\"Advanced text normalization\"\"\"\n","        if pd.isna(text):\n","            return \"\"\n","\n","        text = str(text).lower()\n","\n","        # Apply replacements\n","        for pattern, replacement in self.replacements.items():\n","            text = re.sub(pattern, replacement, text)\n","\n","        # Remove URLs and emails\n","        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' <URL> ', text)\n","        text = re.sub(r'\\S+@\\S+', ' <EMAIL> ', text)\n","\n","        # Remove phone numbers\n","        text = re.sub(r'(\\+84|0)[0-9]{9,10}', ' <PHONE> ', text)\n","\n","        # Normalize whitespace\n","        text = ' '.join(text.split())\n","\n","        return text\n","\n","    def tokenize_vietnamese(self, text):\n","        \"\"\"Vietnamese-aware tokenization\"\"\"\n","        if pd.isna(text) or not text.strip():\n","            return []\n","\n","        # Use Vietnamese tokenizer if available\n","        if UNDERTHESEA_AVAILABLE:\n","            try:\n","                tokens = word_tokenize(text)\n","                return [token for token in tokens if len(token) > 1]\n","            except:\n","                pass\n","\n","        if PYVI_AVAILABLE:\n","            try:\n","                tokenized = ViTokenizer.tokenize(text)\n","                return [token for token in tokenized.split() if len(token) > 1]\n","            except:\n","                pass\n","\n","        # Fallback to simple tokenization\n","        return simple_preprocess(text, min_len=2, max_len=50)\n","\n","    def remove_stopwords(self, tokens):\n","        \"\"\"Remove Vietnamese stopwords\"\"\"\n","        return [token for token in tokens if token not in self.vietnamese_stopwords]\n","\n","    def extract_features(self, text):\n","        \"\"\"Extract linguistic features\"\"\"\n","        features = {\n","            'length': len(str(text)),\n","            'word_count': len(str(text).split()),\n","            'avg_word_length': np.mean([len(word) for word in str(text).split()]) if str(text).split() else 0,\n","            'exclamation_count': str(text).count('!'),\n","            'question_count': str(text).count('?'),\n","            'uppercase_ratio': sum(1 for c in str(text) if c.isupper()) / len(str(text)) if str(text) else 0,\n","            'digit_count': sum(1 for c in str(text) if c.isdigit()),\n","            'fake_indicator_count': sum(1 for word in str(text).lower().split() if word in self.fake_indicators)\n","        }\n","        return features\n","\n","    def clean_and_process(self, text, remove_stopwords=False, extract_features=False):\n","        \"\"\"Complete text processing pipeline\"\"\"\n","        # Clean HTML\n","        clean_text = self.clean_html(text)\n","\n","        # Normalize\n","        normalized = self.normalize_text(clean_text)\n","\n","        # Extract features if requested\n","        features = self.extract_features(normalized) if extract_features else None\n","\n","        # Tokenize\n","        tokens = self.tokenize_vietnamese(normalized)\n","\n","        # Remove stopwords if requested\n","        if remove_stopwords:\n","            tokens = self.remove_stopwords(tokens)\n","\n","        processed_text = ' '.join(tokens)\n","\n","        return processed_text, features\n","\n","print(\"‚úÖ Advanced Vietnamese Text Preprocessor defined!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HhlH8FyQ1s8","executionInfo":{"status":"ok","timestamp":1757172363400,"user_tz":-420,"elapsed":49,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"65e1b85d-b360-4015-e541-d4aa28d09c9f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Advanced Vietnamese Text Preprocessor defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üìã CELL 4: ENHANCED DATASET PREPROCESSOR\n","# ===============================\n","class EnhancedNewsDatasetPreprocessor:\n","    def __init__(self):\n","        self.text_preprocessor = AdvancedVietnameseTextPreprocessor()\n","        self.feature_scaler = StandardScaler()\n","\n","    def combine_text_features(self, df, use_weights=True):\n","        \"\"\"Enhanced text combination with weights\"\"\"\n","        combined_texts = []\n","        linguistic_features = []\n","\n","        # Weights for different text components\n","        weights = {\n","            'title': 3.0,      # Title is most important\n","            'summary': 2.0,    # Summary is very important\n","            'content': 1.0     # Content is baseline\n","        }\n","\n","        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing texts\"):\n","            text_parts = []\n","            row_features = {}\n","\n","            # Process title\n","            if pd.notna(row.get('title')) and str(row.get('title')).strip():\n","                title_clean, title_feat = self.text_preprocessor.clean_and_process(\n","                    row['title'], remove_stopwords=False, extract_features=True\n","                )\n","                if title_clean:\n","                    if use_weights:\n","                        text_parts.extend([title_clean] * int(weights['title']))\n","                    else:\n","                        text_parts.append(title_clean)\n","                    row_features.update({f'title_{k}': v for k, v in title_feat.items()})\n","\n","            # Process summary\n","            if pd.notna(row.get('summary')) and str(row.get('summary')).strip():\n","                summary_clean, summary_feat = self.text_preprocessor.clean_and_process(\n","                    row['summary'], remove_stopwords=False, extract_features=True\n","                )\n","                if summary_clean:\n","                    if use_weights:\n","                        text_parts.extend([summary_clean] * int(weights['summary']))\n","                    else:\n","                        text_parts.append(summary_clean)\n","                    row_features.update({f'summary_{k}': v for k, v in summary_feat.items()})\n","\n","            # Process content\n","            if pd.notna(row.get('content_html')) and str(row.get('content_html')).strip():\n","                content_clean, content_feat = self.text_preprocessor.clean_and_process(\n","                    row['content_html'], remove_stopwords=False, extract_features=True\n","                )\n","                if content_clean:\n","                    text_parts.append(content_clean)\n","                    row_features.update({f'content_{k}': v for k, v in content_feat.items()})\n","\n","            # Combine texts\n","            combined_text = ' [SEP] '.join(text_parts) if text_parts else \"\"\n","            combined_texts.append(combined_text)\n","            linguistic_features.append(row_features)\n","\n","        return combined_texts, linguistic_features\n","\n","    def prepare_data(self, df, min_length=50, max_length=8000, balance_data=False):\n","        \"\"\"Enhanced data preparation with optional balancing\"\"\"\n","        print(\"üìã Enhanced preprocessing...\")\n","\n","        # Combine texts and extract features\n","        combined_texts, linguistic_features = self.combine_text_features(df)\n","        df['combined_text'] = combined_texts\n","\n","        # Add linguistic features to dataframe\n","        if linguistic_features and linguistic_features[0]:  # Check if features exist\n","            feature_df = pd.DataFrame(linguistic_features)\n","            df = pd.concat([df, feature_df], axis=1)\n","\n","        # Filter by text length\n","        df['text_length'] = df['combined_text'].str.len()\n","\n","        print(f\"üìä Original dataset: {len(df)} samples\")\n","\n","        # Remove texts that are too short or too long\n","        df = df[(df['text_length'] >= min_length) & (df['text_length'] <= max_length)]\n","\n","        # Remove empty texts\n","        df = df[df['combined_text'].str.strip() != '']\n","\n","        print(f\"üìä After length filtering: {len(df)} samples\")\n","\n","        # Balance dataset if requested\n","        if balance_data:\n","            df = self.balance_dataset(df)\n","\n","        # Ensure labels are binary\n","        df['label'] = df['label'].astype(int)\n","\n","        print(f\"üìä Final dataset: {len(df)} samples\")\n","        print(f\"üìä Label distribution:\")\n","        print(df['label'].value_counts())\n","        print(f\"üìä Text length statistics:\")\n","        print(df['text_length'].describe())\n","\n","        return df\n","\n","    def balance_dataset(self, df):\n","        \"\"\"Balance dataset using undersampling\"\"\"\n","        print(\"‚öñÔ∏è Balancing dataset...\")\n","\n","        # Get minority class size\n","        label_counts = df['label'].value_counts()\n","        min_size = label_counts.min()\n","\n","        # Sample equal amounts from each class\n","        balanced_dfs = []\n","        for label in df['label'].unique():\n","            class_df = df[df['label'] == label].sample(n=min_size, random_state=42)\n","            balanced_dfs.append(class_df)\n","\n","        balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n","\n","        print(f\"üìä Balanced dataset: {len(balanced_df)} samples\")\n","        print(f\"üìä New label distribution:\")\n","        print(balanced_df['label'].value_counts())\n","\n","        return balanced_df\n","\n","print(\"‚úÖ Enhanced Dataset Preprocessor defined!\")"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:21:33.755136Z","iopub.execute_input":"2024-11-02T07:21:33.755817Z","iopub.status.idle":"2024-11-02T07:21:36.001764Z","shell.execute_reply.started":"2024-11-02T07:21:33.755774Z","shell.execute_reply":"2024-11-02T07:21:36.000854Z"},"trusted":true,"id":"k5MoorOlYSiO","outputId":"fd78675b-0e36-4111-eec8-d3044b4cfda3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757172363456,"user_tz":-420,"elapsed":54,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Enhanced Dataset Preprocessor defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# CELL 5: ü§ñ UNIVERSAL TRANSFORMER CLASSIFIER (fixed imports + metrics logging)\n","# ===============================\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n","import torch.nn as nn\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    acc = accuracy_score(labels, preds)\n","    prec = precision_score(labels, preds, zero_division=0)\n","    rec = recall_score(labels, preds, zero_division=0)\n","    f1 = f1_score(labels, preds, zero_division=0)\n","    try:\n","        auc = roc_auc_score(labels, logits[:, 1])\n","    except:\n","        auc = 0.0\n","    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"auc\": auc}\n","\n","class UniversalTransformerClassifier:\n","    def __init__(self, model_name, max_length=512, num_labels=2, from_scratch=False):\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.num_labels = num_labels\n","        self.from_scratch = from_scratch\n","\n","        print(f\"\\nü§ñ Initializing {model_name} (from_scratch={from_scratch}) ...\")\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","\n","        if from_scratch:\n","            config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n","            self.model = AutoModelForSequenceClassification.from_config(config)\n","            print(f\"‚ö†Ô∏è Model {model_name} initialized with RANDOM weights.\")\n","        else:\n","            self.model = AutoModelForSequenceClassification.from_pretrained(\n","                model_name, num_labels=num_labels\n","            )\n","            print(f\"‚úÖ Model {model_name} loaded pretrained weights.\")\n","\n","        self.model.to(device)\n","\n","    def create_dataset(self, texts, labels=None):\n","        data = {\"text\": texts}\n","        if labels is not None:\n","            data[\"labels\"] = labels\n","        dataset = Dataset.from_dict(data)\n","        return dataset.map(\n","            lambda batch: self.tokenizer(\n","                batch[\"text\"], truncation=True, padding=\"max_length\", max_length=self.max_length\n","            ),\n","            batched=True,\n","        )\n","\n","    def train(self, train_texts, train_labels, val_texts, val_labels,\n","              num_epochs=20, batch_size=16, learning_rate=2e-5):\n","        print(f\"\\nüöÄ Training {self.model_name} for {num_epochs} epochs (from_scratch={self.from_scratch})\")\n","\n","        train_dataset = self.encode(train_texts, train_labels)\n","        val_dataset = self.encode(val_texts, val_labels)\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n","        loss_fn = nn.CrossEntropyLoss()\n","\n","        best_val_f1 = -1.0\n","        best_state, best_preds, best_labels = None, None, None\n","\n","        for epoch in range(1, num_epochs + 1):\n","            # Training\n","            self.model.train()\n","            total_loss = 0.0\n","            for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\", leave=False):\n","                input_ids, attention_mask, labels = [x.to(device) for x in batch]\n","                optimizer.zero_grad()\n","                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs.loss\n","                loss.backward()\n","                optimizer.step()\n","                total_loss += loss.item()\n","\n","            avg_train_loss = total_loss / max(1, len(train_loader))\n","\n","            # Validation\n","            self.model.eval()\n","            val_preds, val_labels_all, val_probs = [], [], []\n","            with torch.no_grad():\n","                for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} - Validation\", leave=False):\n","                    input_ids, attention_mask, labels = [x.to(device) for x in batch]\n","                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","                    probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n","                    preds = np.argmax(probs, axis=1)\n","\n","                    val_preds.extend(preds.tolist())\n","                    val_labels_all.extend(labels.cpu().numpy().tolist())\n","                    val_probs.extend(probs.tolist())\n","\n","            acc = accuracy_score(val_labels_all, val_preds)\n","            prec = precision_score(val_labels_all, val_preds, zero_division=0)\n","            rec = recall_score(val_labels_all, val_preds, zero_division=0)\n","            f1 = f1_score(val_labels_all, val_preds, zero_division=0)\n","            try:\n","                auc = roc_auc_score(val_labels_all, np.array(val_probs)[:, 1])\n","            except:\n","                auc = 0.0\n","\n","            print(f\"üìä Epoch {epoch}/{num_epochs} | Loss={avg_train_loss:.4f} \"\n","                  f\"| Acc={acc:.4f} | Prec={prec:.4f} | Rec={rec:.4f} | F1={f1:.4f} | AUC={auc:.4f}\")\n","\n","            # Save best\n","            if f1 > best_val_f1:\n","                best_val_f1 = f1\n","                best_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}\n","                best_preds, best_labels = val_preds.copy(), val_labels_all.copy()\n","\n","        # Restore best\n","        if best_state is not None:\n","            self.model.load_state_dict(best_state)\n","            self.model.to(device)\n","\n","        # Confusion matrix of best epoch\n","        cm = confusion_matrix(best_labels, best_preds)\n","        plt.figure(figsize=(5,4))\n","        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n","                    xticklabels=[\"Real\",\"Fake\"], yticklabels=[\"Real\",\"Fake\"])\n","        plt.title(f\"Best Confusion Matrix ({self.model_name})\")\n","        plt.xlabel(\"Predicted\")\n","        plt.ylabel(\"True\")\n","        plt.show()\n","\n","        return {\"best_f1\": best_val_f1}\n","\n","    def save_model(self, save_dir=\"./saved_model\"):\n","        os.makedirs(save_dir, exist_ok=True)\n","        self.model.save_pretrained(save_dir)\n","        self.tokenizer.save_pretrained(save_dir)\n","        print(f\"‚úÖ {self.model_name} saved to {save_dir}\")\n","\n","    def predict(self, texts, batch_size=32):\n","        dataset = self.create_dataset(texts)\n","        preds = self.trainer.predict(dataset)\n","        pred_labels = np.argmax(preds.predictions, axis=1)\n","        probs = torch.nn.functional.softmax(torch.tensor(preds.predictions), dim=-1).numpy()\n","        return pred_labels, probs\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:21:39.872125Z","iopub.execute_input":"2024-11-02T07:21:39.872886Z","iopub.status.idle":"2024-11-02T07:21:39.887561Z","shell.execute_reply.started":"2024-11-02T07:21:39.872844Z","shell.execute_reply":"2024-11-02T07:21:39.886649Z"},"trusted":true,"id":"pjB09qYPYSiR","outputId":"864d5ec8-7e06-4c88-948f-3dd2d20cf035","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757172597531,"user_tz":-420,"elapsed":44,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üîß CELL 6 (MODIFIED): PHOBERT FEATURE EXTRACTOR (support from_scratch)\n","# ===============================\n","from transformers import AutoConfig, AutoModel\n","\n","class PhoBERTFeatureExtractor:\n","    def __init__(self, model_name='vinai/phobert-base', layer=-2, from_scratch=False):\n","        self.model_name = model_name\n","        self.layer = layer\n","        self.from_scratch = from_scratch\n","\n","        print(f\"üîß Initializing PhoBERTFeatureExtractor (from_scratch={from_scratch}) ...\")\n","        # Keep tokenizer from pretrained vocab\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        if from_scratch:\n","            config = AutoConfig.from_pretrained(model_name)\n","            # Use AutoModel (encoder only) from config -> random init\n","            self.model = AutoModel.from_config(config)\n","            print(\"‚ö†Ô∏è PhoBERT model initialized with RANDOM weights for feature extraction.\")\n","        else:\n","            # Use pretrained weights (original behavior)\n","            self.model = AutoModel.from_pretrained(model_name)\n","            print(\"‚úÖ PhoBERT pretrained model loaded for feature extraction.\")\n","\n","        self.model.to(device)\n","        self.model.eval()\n","        print(f\"‚úÖ PhoBERT feature extractor ready on {device}\")\n","\n","    def extract_features(self, texts, batch_size=16):\n","        all_features = []\n","        for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting PhoBERT features\"):\n","            batch_texts = texts[i:i+batch_size]\n","            inputs = self.tokenizer(\n","                batch_texts,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                padding=True,\n","                max_length=256\n","            )\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","            with torch.no_grad():\n","                outputs = self.model(**inputs, output_hidden_states=True)\n","                hidden_states = outputs.hidden_states[self.layer]\n","                pooled_features = torch.mean(hidden_states, dim=1)\n","                all_features.extend(pooled_features.cpu().numpy())\n","        return np.array(all_features)\n","\n","print(\"‚úÖ PhoBERT Feature Extractor (with from_scratch option) defined!\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:21:43.492199Z","iopub.execute_input":"2024-11-02T07:21:43.492810Z","iopub.status.idle":"2024-11-02T07:21:43.664903Z","shell.execute_reply.started":"2024-11-02T07:21:43.492773Z","shell.execute_reply":"2024-11-02T07:21:43.664107Z"},"trusted":true,"id":"6-g5JYK7YSiT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757172599437,"user_tz":-420,"elapsed":23,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"0cc784ac-0338-4c12-fafd-aae95c2f0dbd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ PhoBERT Feature Extractor (with from_scratch option) defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üéØ CELL 7 (MODIFIED): HYBRID MODELS (support passing from_scratch to PhoBERT extractor)\n","# ===============================\n","class HybridPhoBERTClassifier:\n","    def __init__(self, method='tfidf', tfidf_features=5000, w2v_size=200, phobert_from_scratch=False):\n","        self.method = method\n","        # Pass the from_scratch flag to feature extractor\n","        self.phobert_extractor = PhoBERTFeatureExtractor(from_scratch=phobert_from_scratch)\n","\n","        if method == 'tfidf':\n","            self.tfidf_vectorizer = TfidfVectorizer(\n","                max_features=tfidf_features,\n","                ngram_range=(1, 3),\n","                lowercase=True,\n","                stop_words=None\n","            )\n","        elif method == 'word2vec':\n","            self.w2v_size = w2v_size\n","            self.word2vec_model = None\n","\n","        self.classifier = LogisticRegression(\n","            random_state=42,\n","            max_iter=1000,\n","            class_weight='balanced',\n","            C=1.0\n","        )\n","        self.feature_scaler = StandardScaler()\n","\n","        print(f\"üéØ Initialized Hybrid PhoBERT + {method.upper()} classifier (phobert_from_scratch={phobert_from_scratch})\")\n","\n","    # prepare_word2vec_features, train, predict - keep the same as your original implementation\n","    def prepare_word2vec_features(self, texts):\n","        print(\"üî§ Preparing Word2Vec features...\")\n","        tokenized_texts = []\n","        preprocessor = AdvancedVietnameseTextPreprocessor()\n","        for text in tqdm(texts, desc=\"Tokenizing for Word2Vec\"):\n","            tokens = preprocessor.tokenize_vietnamese(text)\n","            if tokens:\n","                tokenized_texts.append(tokens)\n","        if self.word2vec_model is None:\n","            print(\"üî§ Training Word2Vec model...\")\n","            self.word2vec_model = Word2Vec(\n","                sentences=tokenized_texts,\n","                vector_size=self.w2v_size,\n","                window=5,\n","                min_count=2,\n","                workers=4,\n","                epochs=10,\n","                sg=1\n","            )\n","        doc_vectors = []\n","        for tokens in tokenized_texts:\n","            vectors = []\n","            for token in tokens:\n","                if token in self.word2vec_model.wv:\n","                    vectors.append(self.word2vec_model.wv[token])\n","            if vectors:\n","                doc_vector = np.mean(vectors, axis=0)\n","            else:\n","                doc_vector = np.zeros(self.w2v_size)\n","            doc_vectors.append(doc_vector)\n","        return np.array(doc_vectors)\n","\n","    def train(self, train_texts, train_labels):\n","        print(f\"üîß Training PhoBERT + {self.method.upper()}...\")\n","        phobert_features = self.phobert_extractor.extract_features(train_texts)\n","        if self.method == 'tfidf':\n","            text_features = self.tfidf_vectorizer.fit_transform(train_texts).toarray()\n","        elif self.method == 'word2vec':\n","            text_features = self.prepare_word2vec_features(train_texts)\n","        combined_features = np.hstack([phobert_features, text_features])\n","        combined_features_scaled = self.feature_scaler.fit_transform(combined_features)\n","        self.classifier.fit(combined_features_scaled, train_labels)\n","        print(f\"‚úÖ PhoBERT + {self.method.upper()} training completed!\")\n","        return self\n","\n","    def predict(self, texts):\n","        phobert_features = self.phobert_extractor.extract_features(texts)\n","        if self.method == 'tfidf':\n","            text_features = self.tfidf_vectorizer.transform(texts).toarray()\n","        elif self.method == 'word2vec':\n","            text_features = self.prepare_word2vec_features(texts)\n","        combined_features = np.hstack([phobert_features, text_features])\n","        combined_features_scaled = self.feature_scaler.transform(combined_features)\n","        predictions = self.classifier.predict(combined_features_scaled)\n","        probabilities = self.classifier.predict_proba(combined_features_scaled)\n","        return predictions, probabilities\n","\n","print(\"‚úÖ Hybrid PhoBERT Classifiers (with phobert_from_scratch option) defined!\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:21:47.384468Z","iopub.execute_input":"2024-11-02T07:21:47.384821Z","iopub.status.idle":"2024-11-02T07:21:47.432656Z","shell.execute_reply.started":"2024-11-02T07:21:47.384790Z","shell.execute_reply":"2024-11-02T07:21:47.431925Z"},"trusted":true,"id":"Yu4Fo31_YSiU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757172601375,"user_tz":-420,"elapsed":17,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"45e27cea-cf2e-4408-9265-9147175792b2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Hybrid PhoBERT Classifiers (with phobert_from_scratch option) defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üìä CELL 8: COMPREHENSIVE MODEL EVALUATOR\n","# ===============================\n","class ComprehensiveModelEvaluator:\n","    def __init__(self):\n","        self.results = {}\n","        self.predictions = {}\n","        self.probabilities = {}\n","\n","    def evaluate_model(self, y_true, y_pred, y_prob=None, model_name=\"Model\"):\n","        \"\"\"Comprehensive model evaluation\"\"\"\n","\n","        # Basic metrics\n","        results = {\n","            'accuracy': accuracy_score(y_true, y_pred),\n","            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n","            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n","            'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n","            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n","            'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0)\n","        }\n","\n","        # AUC if probabilities are provided\n","        if y_prob is not None and y_prob.shape[1] == 2:\n","            try:\n","                results['auc'] = roc_auc_score(y_true, y_prob[:, 1])\n","            except:\n","                results['auc'] = 0.0\n","\n","        self.results[model_name] = results\n","        self.predictions[model_name] = y_pred\n","        if y_prob is not None:\n","            self.probabilities[model_name] = y_prob\n","\n","        print(f\"\\nüìä {model_name} Results:\")\n","        print(\"-\" * 50)\n","        for metric, value in results.items():\n","            print(f\"{metric.upper():15}: {value:.4f}\")\n","\n","        # Detailed classification report\n","        print(f\"\\nüìã Classification Report - {model_name}:\")\n","        print(classification_report(y_true, y_pred, target_names=['Real', 'Fake'], digits=4))\n","\n","        return results\n","\n","    def plot_confusion_matrices(self, y_true, models_to_plot=None):\n","        \"\"\"Plot confusion matrices for all models\"\"\"\n","        if models_to_plot is None:\n","            models_to_plot = list(self.predictions.keys())\n","\n","        n_models = len(models_to_plot)\n","        if n_models == 0:\n","            return\n","\n","        # Calculate grid dimensions\n","        n_cols = min(3, n_models)\n","        n_rows = (n_models + n_cols - 1) // n_cols\n","\n","        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n","        if n_models == 1:\n","            axes = [axes]\n","        elif n_rows == 1:\n","            axes = axes.reshape(1, -1)\n","\n","        for idx, model_name in enumerate(models_to_plot):\n","            row = idx // n_cols\n","            col = idx % n_cols\n","            ax = axes[row, col] if n_rows > 1 else axes[col]\n","\n","            cm = confusion_matrix(y_true, self.predictions[model_name])\n","\n","            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n","                       xticklabels=['Real', 'Fake'],\n","                       yticklabels=['Real', 'Fake'])\n","            ax.set_title(f'{model_name}')\n","            ax.set_ylabel('True Label')\n","            ax.set_xlabel('Predicted Label')\n","\n","        # Hide empty subplots\n","        for idx in range(n_models, n_rows * n_cols):\n","            if n_rows > 1:\n","                row = idx // n_cols\n","                col = idx % n_cols\n","                fig.delaxes(axes[row, col])\n","            elif n_cols > 1:\n","                fig.delaxes(axes[idx])\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def plot_roc_curves(self, y_true):\n","        \"\"\"Plot ROC curves for models with probabilities\"\"\"\n","        plt.figure(figsize=(10, 8))\n","\n","        for model_name, y_prob in self.probabilities.items():\n","            if y_prob.shape[1] == 2:  # Binary classification\n","                fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n","                auc_score = roc_auc_score(y_true, y_prob[:, 1])\n","                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n","\n","        plt.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('ROC Curves Comparison')\n","        plt.legend(loc=\"lower right\")\n","        plt.grid(True, alpha=0.3)\n","        plt.show()\n","\n","    def compare_models(self, plot_confusion=True, plot_roc=True):\n","        \"\"\"Comprehensive model comparison\"\"\"\n","        if not self.results:\n","            print(\"No results to compare!\")\n","            return None\n","\n","        df_results = pd.DataFrame(self.results).T\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"üèÜ COMPREHENSIVE MODEL COMPARISON\")\n","        print(\"=\"*80)\n","        print(df_results.round(4))\n","\n","        # Plot metrics comparison\n","        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","        fig.suptitle('Model Performance Comparison', fontsize=16)\n","\n","        # Accuracy, Precision, Recall, F1\n","        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_weighted']\n","        colors = ['skyblue', 'lightgreen', 'salmon', 'gold']\n","\n","        for i, metric in enumerate(metrics_to_plot):\n","            ax = axes[i//2, i%2]\n","            bars = ax.bar(df_results.index, df_results[metric], color=colors[i])\n","            ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n","            ax.set_ylabel('Score')\n","            ax.set_ylim(0, 1)\n","\n","            # Add value labels on bars\n","            for bar in bars:\n","                height = bar.get_height()\n","                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n","                       f'{height:.3f}', ha='center', va='bottom')\n","\n","            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Plot confusion matrices if requested\n","        if plot_confusion and self.predictions:\n","            y_true_sample = None\n","            for pred in self.predictions.values():\n","                y_true_sample = pred  # We'll need the actual y_true from outside\n","                break\n","            # Note: This will be called with actual y_true from the main pipeline\n","\n","        return df_results\n","\n","    def get_best_model(self, metric='f1_weighted'):\n","        \"\"\"Get the best performing model\"\"\"\n","        if not self.results:\n","            return None\n","\n","        best_score = -1\n","        best_model = None\n","\n","        for model_name, results in self.results.items():\n","            if results.get(metric, 0) > best_score:\n","                best_score = results[metric]\n","                best_model = model_name\n","\n","        print(f\"üèÖ Best model: {best_model} with {metric} = {best_score:.4f}\")\n","        return best_model, best_score\n","\n","print(\"‚úÖ Comprehensive Model Evaluator defined!\")"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:21:54.201672Z","iopub.execute_input":"2024-11-02T07:21:54.202435Z","iopub.status.idle":"2024-11-02T07:33:45.389705Z","shell.execute_reply.started":"2024-11-02T07:21:54.202394Z","shell.execute_reply":"2024-11-02T07:33:45.388797Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8O7L0AwuYSiW","outputId":"237b45d8-0f0c-42d4-b689-37a72611f52d","executionInfo":{"status":"ok","timestamp":1757172603428,"user_tz":-420,"elapsed":90,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Comprehensive Model Evaluator defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üîÑ CELL 9: MAIN PIPELINE CLASS (updated for from_scratch training)\n","# ===============================\n","class ComprehensiveFakeNewsPipeline:\n","    def __init__(self):\n","        self.preprocessor = EnhancedNewsDatasetPreprocessor()\n","        self.evaluator = ComprehensiveModelEvaluator()\n","        self.models = {}\n","        self.full_data = None   # ƒë·ªÉ l∆∞u dataset ƒë·∫ßy ƒë·ªß\n","\n","    def load_data(self, fake_path, real_path):\n","        \"\"\"Load and combine datasets\"\"\"\n","        print(\"üìÇ Loading datasets...\")\n","        cols = ['domain','title','publish_date','summary','content_html','label']\n","        try:\n","            df_fake = pd.read_csv(fake_path, usecols=cols)\n","            df_real = pd.read_csv(real_path, usecols=cols)\n","        except Exception as e:\n","            print(f\"‚ùå Error loading data: {e}\")\n","            return None\n","\n","        print(f\"üìä Fake news articles: {len(df_fake)}\")\n","        print(f\"üìä Real news articles: {len(df_real)}\")\n","\n","        df = pd.concat([df_fake, df_real], axis=0).reset_index(drop=True)\n","\n","        print(f\"üìä Combined dataset: {len(df)} articles\")\n","        print(f\"üìä Label distribution:\")\n","        print(df['label'].value_counts())\n","\n","        return df\n","\n","    def run_comprehensive_pipeline(self, df, test_size=0.2,\n","                                   balance_data=False, models_to_run=None, keep_full_data=True):\n","        \"\"\"Run the complete pipeline\"\"\"\n","\n","        # Gi·ªØ dataset g·ªëc\n","        full_df = df.copy()\n","\n","        # Ti·ªÅn x·ª≠ l√Ω\n","        df_clean = self.preprocessor.prepare_data(df, balance_data=balance_data)\n","        if len(df_clean) == 0:\n","            print(\"‚ùå No data after preprocessing!\")\n","            return None\n","\n","        # N·∫øu mu·ªën gi·ªØ dataset ƒë·∫ßy ƒë·ªß\n","        if keep_full_data:\n","            self.full_data = self.preprocessor.prepare_data(full_df, balance_data=False)\n","        else:\n","            self.full_data = df_clean\n","\n","        # Train-test split\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            df_clean['combined_text'].tolist(),\n","            df_clean['label'].tolist(),\n","            test_size=test_size,\n","            random_state=42,\n","            stratify=df_clean['label']\n","        )\n","\n","        print(f\"\\nüìä Dataset splits:\")\n","        print(f\"Training samples: {len(X_train)}\")\n","        print(f\"Test samples: {len(X_test)}\")\n","        print(f\"Train label distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n","        print(f\"Test label distribution: {pd.Series(y_test).value_counts().to_dict()}\")\n","\n","        # Run selected models\n","        results_summary = {}\n","        num_epochs = 20\n","        batch_size = 16\n","\n","        # 1. PhoBERT\n","        if 'phobert' in models_to_run:\n","            print(\"\\n\" + \"=\"*60)\n","            print(\"ü§ñ TRAINING PHOBERT (from scratch)\")\n","            print(\"=\"*60)\n","\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","            phobert = UniversalTransformerClassifier('vinai/phobert-base', max_length=256, from_scratch=True)\n","\n","            phobert.train(X_train, y_train, X_test, y_test,\n","                          num_epochs=num_epochs, batch_size=batch_size)\n","\n","            phobert_pred, phobert_prob = phobert.predict(X_test, batch_size=batch_size)\n","\n","            results_summary['PhoBERT'] = self.evaluator.evaluate_model(\n","                y_test, phobert_pred, phobert_prob, \"PhoBERT\"\n","            )\n","            self.models['PhoBERT'] = phobert\n","\n","        # 2. BERT\n","        if 'bert' in models_to_run:\n","            print(\"\\n\" + \"=\"*60)\n","            print(\"ü§ñ TRAINING BERT (from scratch)\")\n","            print(\"=\"*60)\n","\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","            bert = UniversalTransformerClassifier('bert-base-multilingual-cased', max_length=512, from_scratch=True)\n","\n","            bert.train(X_train, y_train, X_test, y_test,\n","                       num_epochs=num_epochs, batch_size=batch_size)\n","\n","            bert_pred, bert_prob = bert.predict(X_test, batch_size=batch_size)\n","\n","            results_summary['BERT'] = self.evaluator.evaluate_model(\n","                y_test, bert_pred, bert_prob, \"BERT\"\n","            )\n","            self.models['BERT'] = bert\n","\n","        # 3. RoBERTa\n","        if 'roberta' in models_to_run:\n","            print(\"\\n\" + \"=\"*60)\n","            print(\"ü§ñ TRAINING ROBERTA (from scratch)\")\n","            print(\"=\"*60)\n","\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","            roberta = UniversalTransformerClassifier('roberta-base', max_length=512, from_scratch=True)\n","\n","            roberta.train(X_train, y_train, X_test, y_test,\n","                          num_epochs=num_epochs, batch_size=batch_size)\n","\n","            roberta_pred, roberta_prob = roberta.predict(X_test, batch_size=batch_size)\n","\n","            results_summary['RoBERTa'] = self.evaluator.evaluate_model(\n","                y_test, roberta_pred, roberta_prob, \"RoBERTa\"\n","            )\n","            self.models['RoBERTa'] = roberta\n","\n","        # 4. PhoBERT + TF-IDF\n","        if 'phobert_tfidf' in models_to_run:\n","            print(\"\\n\" + \"=\"*60)\n","            print(\"üéØ TRAINING PHOBERT + TF-IDF (from scratch)\")\n","            print(\"=\"*60)\n","\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","            phobert_tfidf = HybridPhoBERTClassifier(method='tfidf', tfidf_features=10000, phobert_from_scratch=True)\n","            phobert_tfidf.train(X_train, y_train)\n","\n","            tfidf_pred, tfidf_prob = phobert_tfidf.predict(X_test)\n","\n","            results_summary['PhoBERT+TF-IDF'] = self.evaluator.evaluate_model(\n","                y_test, tfidf_pred, tfidf_prob, \"PhoBERT+TF-IDF\"\n","            )\n","            self.models['PhoBERT+TF-IDF'] = phobert_tfidf\n","\n","        # 5. PhoBERT + Word2Vec\n","        if 'phobert_w2v' in models_to_run:\n","            print(\"\\n\" + \"=\"*60)\n","            print(\"üéØ TRAINING PHOBERT + WORD2VEC (from scratch)\")\n","            print(\"=\"*60)\n","\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","            phobert_w2v = HybridPhoBERTClassifier(method='word2vec', w2v_size=300, phobert_from_scratch=True)\n","            phobert_w2v.train(X_train, y_train)\n","\n","            w2v_pred, w2v_prob = phobert_w2v.predict(X_test)\n","\n","            results_summary['PhoBERT+Word2Vec'] = self.evaluator.evaluate_model(\n","                y_test, w2v_pred, w2v_prob, \"PhoBERT+Word2Vec\"\n","            )\n","            self.models['PhoBERT+Word2Vec'] = phobert_w2v\n","\n","        # Final comparison\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"üèÜ FINAL RESULTS COMPARISON\")\n","        print(\"=\"*80)\n","\n","        comparison_df = self.evaluator.compare_models()\n","\n","        # Plot confusion matrices\n","        if len(self.evaluator.predictions) > 0:\n","            print(\"\\nüìä Confusion Matrices:\")\n","            self.evaluator.plot_confusion_matrices(y_test)\n","\n","        # Plot ROC curves\n","        if len(self.evaluator.probabilities) > 0:\n","            print(\"\\nüìà ROC Curves:\")\n","            self.evaluator.plot_roc_curves(y_test)\n","\n","        # Get best model\n","        best_model, best_score = self.evaluator.get_best_model('f1_weighted')\n","\n","        return {\n","            'comparison_df': comparison_df,\n","            'best_model': best_model,\n","            'best_score': best_score,\n","            'X_test': X_test,\n","            'y_test': y_test,\n","            'models': self.models,\n","            'evaluator': self.evaluator\n","        }\n","\n","print(\"‚úÖ Comprehensive Pipeline (from scratch) defined!\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:37:35.792315Z","iopub.execute_input":"2024-11-02T07:37:35.792674Z","iopub.status.idle":"2024-11-02T07:37:35.797727Z","shell.execute_reply.started":"2024-11-02T07:37:35.792642Z","shell.execute_reply":"2024-11-02T07:37:35.796797Z"},"trusted":true,"id":"_Xz9Row7YSiY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757172606424,"user_tz":-420,"elapsed":33,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"23ee909c-a3b3-4b9a-cbaa-d8a7cbb10a3b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Comprehensive Pipeline (from scratch) defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üéØ CELL 10: PREDICTION & UTILITY FUNCTIONS\n","# ===============================\n","def predict_single_text(pipeline, text, model_name=None):\n","    \"\"\"Predict on single text with the best model or specified model\"\"\"\n","\n","    if model_name is None:\n","        # Use best model\n","        best_model, _ = pipeline.evaluator.get_best_model('f1_weighted')\n","        model_name = best_model\n","\n","    if model_name not in pipeline.models:\n","        print(f\"‚ùå Model {model_name} not found!\")\n","        available_models = list(pipeline.models.keys())\n","        print(f\"Available models: {available_models}\")\n","        return None\n","\n","    model = pipeline.models[model_name]\n","    pred, prob = model.predict([text])\n","\n","    label = \"FAKE\" if pred[0] == 1 else \"REAL\"\n","    confidence = max(prob[0])\n","\n","    print(f\"\\nüìù Text: {text[:200]}...\")\n","    print(f\"üéØ Model: {model_name}\")\n","    print(f\"‚úÖ Prediction: {label} (Confidence={confidence:.4f})\")\n","    print(f\"üìä Probabilities ‚Üí Real={prob[0][0]:.4f}, Fake={prob[0][1]:.4f}\")\n","\n","    return pred[0], confidence, prob[0]\n","\n","\n","def save_models(pipeline, base_path=\"/content/drive/MyDrive/fake_news_models\"):\n","    \"\"\"Save all trained models\"\"\"\n","    os.makedirs(base_path, exist_ok=True)\n","\n","    for model_name, model in pipeline.models.items():\n","        try:\n","            if hasattr(model, 'model') and hasattr(model, 'tokenizer'):\n","                # Transformer models\n","                model_path = os.path.join(base_path, model_name.lower().replace('+', '_'))\n","                os.makedirs(model_path, exist_ok=True)\n","                model.model.save_pretrained(model_path)\n","                model.tokenizer.save_pretrained(model_path)\n","                print(f\"‚úÖ {model_name} saved to {model_path}\")\n","            else:\n","                # Hybrid models\n","                model_path = os.path.join(base_path, f\"{model_name.lower().replace('+', '_')}.joblib\")\n","                joblib.dump(model, model_path)\n","                print(f\"‚úÖ {model_name} saved to {model_path}\")\n","        except Exception as e:\n","            print(f\"‚ùå Error saving {model_name}: {e}\")\n","\n","\n","def analyze_dataset_statistics(df):\n","    \"\"\"Analyze dataset statistics\"\"\"\n","    print(\"üìä DATASET ANALYSIS\")\n","    print(\"=\"*50)\n","\n","    # Basic statistics\n","    print(f\"Total articles: {len(df)}\")\n","    print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n","    print(f\"Label percentage:\\n{df['label'].value_counts(normalize=True).round(4)}\")\n","\n","    # Text length analysis\n","    if 'combined_text' in df.columns:\n","        print(f\"\\nText length statistics:\\n{df['combined_text'].str.len().describe()}\")\n","\n","    # Missing values\n","    print(f\"\\nMissing values:\")\n","    for col in ['title', 'summary', 'content_html']:\n","        if col in df.columns:\n","            missing = df[col].isna().sum()\n","            print(f\"{col}: {missing} ({missing/len(df)*100:.1f}%)\")\n","\n","    # Domain analysis if available\n","    if 'domain' in df.columns:\n","        print(f\"\\nTop 10 domains:\\n{df['domain'].value_counts().head(10)}\")\n","\n","print(\"‚úÖ Utility functions defined!\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:37:41.051938Z","iopub.execute_input":"2024-11-02T07:37:41.052338Z","iopub.status.idle":"2024-11-02T07:37:41.477222Z","shell.execute_reply.started":"2024-11-02T07:37:41.052300Z","shell.execute_reply":"2024-11-02T07:37:41.476235Z"},"trusted":true,"id":"k_63KncNYSia","outputId":"24fbe4a4-73d8-476c-bef9-a435f9a30c56","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757172609434,"user_tz":-420,"elapsed":53,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Utility functions defined!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# üöÄ CELL 11: MAIN EXECUTION\n","# ===============================\n","import pandas as pd\n","import numpy as np\n","import torch\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","def main(sample_size=None, models_to_run=None, balance_data=False, keep_full_data=True):\n","    \"\"\"Main function to run the complete pipeline\"\"\"\n","\n","    if models_to_run is None:\n","        # Default: run all models\n","        models_to_run = ['phobert', 'bert', 'roberta', 'phobert_tfidf', 'phobert_w2v']\n","\n","    print(\"üöÄ STARTING COMPREHENSIVE FAKE NEWS DETECTION PIPELINE\")\n","    print(\"=\"*80)\n","\n","    # Initialize pipeline\n","    pipeline = ComprehensiveFakeNewsPipeline()\n","\n","    # Load data - UPDATE THESE PATHS\n","    fake_path = \"/content/drive/MyDrive/NLP Project - Thay Khanh/ViFN-Vietnamese_Fake_New_Datasets_Ver3-main/processed/deduplicated_articles_fake.csv\"\n","    real_path = \"/content/drive/MyDrive/NLP Project - Thay Khanh/ViFN-Vietnamese_Fake_New_Datasets_Ver3-main/processed/deduplicated_articles_real.csv\"\n","\n","    df = pipeline.load_data(fake_path, real_path)\n","    if df is None:\n","        print(\"‚ùå Failed to load data. Please check your file paths!\")\n","        return None\n","\n","    # Analyze dataset\n","    analyze_dataset_statistics(df)\n","\n","    # Run pipeline\n","    print(\"\\n‚öôÔ∏è Configuration:\")\n","    print(f\"üìä Sample size: {'Full dataset' if sample_size is None else sample_size}\")\n","    print(f\"ü§ñ Models to run: {models_to_run}\")\n","    print(f\"‚öñÔ∏è Balance data: {balance_data}\")\n","    print(f\"üìã Keep full data: {keep_full_data}\")\n","\n","    results = pipeline.run_comprehensive_pipeline(\n","        df,\n","        balance_data=balance_data,\n","        models_to_run=models_to_run,\n","        keep_full_data=keep_full_data\n","    )\n","\n","    if results is None:\n","        print(\"‚ùå Pipeline failed!\")\n","        return None\n","\n","    print(\"\\n‚úÖ Pipeline completed successfully!\")\n","    return pipeline, results\n"],"metadata":{"execution":{"iopub.status.busy":"2024-11-02T07:37:45.734564Z","iopub.execute_input":"2024-11-02T07:37:45.734940Z","iopub.status.idle":"2024-11-02T07:37:45.744475Z","shell.execute_reply.started":"2024-11-02T07:37:45.734905Z","shell.execute_reply":"2024-11-02T07:37:45.743554Z"},"trusted":true,"id":"uk1zi6ByYSic","executionInfo":{"status":"ok","timestamp":1757172611755,"user_tz":-420,"elapsed":10,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# üéØ CELL 12: RUN THE PIPELINE\n","# ===============================\n","\n","# Configuration - MODIFY THESE AS NEEDED\n","SAMPLE_SIZE = None  # Full dataset\n","MODELS_TO_RUN = ['phobert', 'bert', 'roberta', 'phobert_tfidf', 'phobert_w2v']\n","\n","# Run\n","pipeline, results = main(\n","    sample_size=SAMPLE_SIZE,\n","    models_to_run=MODELS_TO_RUN,\n","    balance_data=False,   # Set True n·∫øu d·ªØ li·ªáu qu√° l·ªách\n","    keep_full_data=True\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hjU2F7wu91YM","executionInfo":{"status":"error","timestamp":1757173978298,"user_tz":-420,"elapsed":1364416,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}},"outputId":"745b3598-3a30-47ba-e584-f883fa912c56"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STARTING COMPREHENSIVE FAKE NEWS DETECTION PIPELINE\n","================================================================================\n","üìÇ Loading datasets...\n","üìä Fake news articles: 666\n","üìä Real news articles: 4795\n","üìä Combined dataset: 5461 articles\n","üìä Label distribution:\n","label\n","0    4795\n","1     666\n","Name: count, dtype: int64\n","üìä DATASET ANALYSIS\n","==================================================\n","Total articles: 5461\n","Label distribution:\n","label\n","0    4795\n","1     666\n","Name: count, dtype: int64\n","Label percentage:\n","label\n","0    0.878\n","1    0.122\n","Name: proportion, dtype: float64\n","\n","Missing values:\n","title: 0 (0.0%)\n","summary: 17 (0.3%)\n","content_html: 0 (0.0%)\n","\n","Top 10 domains:\n","domain\n","baomoi.com       929\n","vietnamnet.vn    745\n","hanoimoi.vn      564\n","tienphong.vn     477\n","laodong.vn       367\n","tuoitre.vn       325\n","vov.vn           316\n","vnexpress.net    314\n","baocaobang.vn    297\n","thanhnien.vn     185\n","Name: count, dtype: int64\n","\n","‚öôÔ∏è Configuration:\n","üìä Sample size: Full dataset\n","ü§ñ Models to run: ['phobert', 'bert', 'roberta', 'phobert_tfidf', 'phobert_w2v']\n","‚öñÔ∏è Balance data: False\n","üìã Keep full data: True\n","üìã Enhanced preprocessing...\n"]},{"output_type":"stream","name":"stderr","text":["Processing texts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5461/5461 [01:48<00:00, 50.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["üìä Original dataset: 5461 samples\n","üìä After length filtering: 4888 samples\n","üìä Final dataset: 4888 samples\n","üìä Label distribution:\n","label\n","0    4251\n","1     637\n","Name: count, dtype: int64\n","üìä Text length statistics:\n","count    4888.000000\n","mean     3530.451718\n","std      1597.710313\n","min       183.000000\n","25%      2338.750000\n","50%      3303.500000\n","75%      4539.250000\n","max      7998.000000\n","Name: text_length, dtype: float64\n","üìã Enhanced preprocessing...\n"]},{"output_type":"stream","name":"stderr","text":["Processing texts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5461/5461 [01:56<00:00, 46.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["üìä Original dataset: 5461 samples\n","üìä After length filtering: 4888 samples\n","üìä Final dataset: 4888 samples\n","üìä Label distribution:\n","label\n","0    4251\n","1     637\n","Name: count, dtype: int64\n","üìä Text length statistics:\n","count    4888.000000\n","mean     3530.451718\n","std      1597.710313\n","min       183.000000\n","25%      2338.750000\n","50%      3303.500000\n","75%      4539.250000\n","max      7998.000000\n","Name: text_length, dtype: float64\n","\n","üìä Dataset splits:\n","Training samples: 3910\n","Test samples: 978\n","Train label distribution: {0: 3400, 1: 510}\n","Test label distribution: {0: 851, 1: 127}\n","\n","============================================================\n","ü§ñ TRAINING PHOBERT (from scratch)\n","============================================================\n","\n","ü§ñ Initializing vinai/phobert-base (from_scratch=True) ...\n","‚ö†Ô∏è Model vinai/phobert-base initialized with RANDOM weights.\n","\n","üöÄ Training vinai/phobert-base for 20 epochs (from_scratch=True)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üìä Epoch 1/20 | Loss=0.3983 | Acc=0.8701 | Prec=0.0000 | Rec=0.0000 | F1=0.0000 | AUC=0.8470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üìä Epoch 2/20 | Loss=0.2730 | Acc=0.9407 | Prec=0.7447 | Rec=0.8268 | F1=0.7836 | AUC=0.9669\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üìä Epoch 3/20 | Loss=0.1029 | Acc=0.9571 | Prec=0.9126 | Rec=0.7402 | F1=0.8174 | AUC=0.9796\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üìä Epoch 4/20 | Loss=0.0673 | Acc=0.9622 | Prec=0.9592 | Rec=0.7402 | F1=0.8356 | AUC=0.9685\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üìä Epoch 5/20 | Loss=0.0415 | Acc=0.9581 | Prec=0.9674 | Rec=0.7008 | F1=0.8128 | AUC=0.9833\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["üìä Epoch 6/20 | Loss=0.0186 | Acc=0.9376 | Prec=1.0000 | Rec=0.5197 | F1=0.6839 | AUC=0.9677\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-220402666.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m pipeline, results = main(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodels_to_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODELS_TO_RUN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-397472769.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sample_size, models_to_run, balance_data, keep_full_data)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìã Keep full data: {keep_full_data}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     results = pipeline.run_comprehensive_pipeline(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mbalance_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbalance_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1754689280.py\u001b[0m in \u001b[0;36mrun_comprehensive_pipeline\u001b[0;34m(self, df, test_size, balance_data, models_to_run, keep_full_data)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mphobert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniversalTransformerClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vinai/phobert-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_scratch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             phobert.train(X_train, y_train, X_test, y_test, \n\u001b[0m\u001b[1;32m     82\u001b[0m                           num_epochs=num_epochs, batch_size=batch_size)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-983965241.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_texts, train_labels, val_texts, val_labels, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","def evaluate_test_samples(pipeline, test_texts, true_labels):\n","    \"\"\"\n","    Evaluate multiple test samples across all trained models.\n","    Show comparison table: True label + predictions of each model.\n","    \"\"\"\n","    results = []\n","    models = list(pipeline.models.keys())  # ['PhoBERT', 'BERT', 'RoBERTa', 'PhoBERT+TF-IDF', 'PhoBERT+Word2Vec']\n","\n","    for i, text in enumerate(test_texts, 1):\n","        row = {\n","            \"Index\": i,\n","            \"Post/News\": text[:120] + (\"...\" if len(text) > 120 else \"\"),\n","            \"True Label\": \"FAKE\" if true_labels[i-1] == 1 else \"REAL\"\n","        }\n","        for model_name in models:\n","            model = pipeline.models[model_name]\n","            pred, prob = model.predict([text])\n","            row[model_name] = \"FAKE\" if pred[0] == 1 else \"REAL\"\n","        results.append(row)\n","\n","    df_results = pd.DataFrame(results)\n","    print(\"\\nüìä Comparison Table across models:\")\n","    display(df_results)\n","    return df_results\n","\n","\n","# ===============================\n","# üß™ CELL 13: TEST PREDICTIONS (UPDATED)\n","# ===============================\n","if pipeline is not None:\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"üß™ TESTING PREDICTIONS (Comparison Table)\")\n","    print(\"=\"*50)\n","\n","    # 4 test samples\n","    test_texts = [\n","        \"Ch√≠nh ph·ªß Vi·ªát Nam c√¥ng b·ªë ch√≠nh s√°ch m·ªõi v·ªÅ ph√°t tri·ªÉn kinh t·∫ø s·ªë trong nƒÉm 2024\",\n","        \"N√ìNG: Ph√°t hi·ªán lo·∫°i th·ª±c ph·∫©m c√≥ th·ªÉ ch·ªØa kh·ªèi m·ªçi b·ªánh ung th∆∞ ch·ªâ trong 3 ng√†y!\",\n","        \"Nghi√™n c·ª©u khoa h·ªçc m·ªõi cho th·∫•y t√°c ƒë·ªông t√≠ch c·ª±c c·ªßa vi·ªác t·∫≠p th·ªÉ d·ª•c ƒë·ªëi v·ªõi s·ª©c kh·ªèe\",\n","        \"SHOCK: Tr√°i ƒë·∫•t s·∫Ω k·∫øt th√∫c v√†o nƒÉm 2025 theo l·ªùi ti√™n tri c·ªï ƒë·∫°i!\"\n","    ]\n","\n","    # True labels for reference (1=FAKE, 0=REAL)\n","    true_labels = [1, 1, 0, 0]\n","\n","    results_table = evaluate_test_samples(pipeline, test_texts, true_labels)\n","\n","print(\"\\nüéâ ALL DONE! Table ready for your report.\")\n"],"metadata":{"id":"pkqTS48QDZp0","executionInfo":{"status":"aborted","timestamp":1757172363519,"user_tz":-420,"elapsed":5,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import joblib\n","import torch\n","\n","# ===============================\n","# üì¶ L∆ØU TF-IDF + LogisticRegression\n","# ===============================\n","def save_tfidf_model(tfidf_model, save_path=\"/content/drive/MyDrive/fake_news_models/tfidf_model.joblib\"):\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","    joblib.dump(tfidf_model, save_path)\n","    print(f\"‚úÖ TF-IDF model saved to {save_path}\")\n","\n","# ===============================\n","# ü§ñ L∆ØU PHOBERT (HuggingFace)\n","# ===============================\n","def save_phobert_model(phobert_model, save_dir=\"/content/drive/MyDrive/fake_news_models/phobert_model\"):\n","    os.makedirs(save_dir, exist_ok=True)\n","    phobert_model.model.save_pretrained(save_dir)\n","    phobert_model.tokenizer.save_pretrained(save_dir)\n","    print(f\"‚úÖ PhoBERT model saved to {save_dir}\")\n"],"metadata":{"id":"6PfI7VBQNR9C","executionInfo":{"status":"aborted","timestamp":1757172363521,"user_tz":-420,"elapsed":57672,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# L∆∞u TF-IDF\n","if 'TF-IDF' in pipeline.models:\n","    save_tfidf_model(pipeline.models['TF-IDF'])\n","\n","# L∆∞u PhoBERT\n","if 'PhoBERT' in pipeline.models:\n","    save_phobert_model(pipeline.models['PhoBERT'])\n"],"metadata":{"id":"dIopv7svNV3r","executionInfo":{"status":"aborted","timestamp":1757172363535,"user_tz":-420,"elapsed":3,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# üìù USAGE INSTRUCTIONS\n","# ===============================\n","print(\"\"\"\n","üìù H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:\n","\n","1. üìÅ ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n file CSV ch√≠nh x√°c trong CELL 11\n","\n","2. ‚ñ∂Ô∏è Ch·∫°y c√°c cells theo th·ª© t·ª±:\n","   - CELL 1-10: Setup v√† ƒë·ªãnh nghƒ©a classes\n","   - CELL 11: Load dataset (theo c√°ch b·∫°n ƒë√£ vi·∫øt)\n","   - CELL 12: ƒê·ªãnh nghƒ©a main function\n","   - CELL 13: Ch·∫°y pipeline\n","   - CELL 14: Test predictions (optional)\n","\n","3. üîß T√πy ch·ªânh trong CELL 13:\n","   - sample_size=2000: Test nhanh\n","   - sample_size=None: D√πng full dataset\n","   - run_phobert=False: B·ªè qua PhoBERT n·∫øu mu·ªën\n","\n","4. üß™ Test predictions:\n","   predict_single_text(pipeline, \"Your text here\", \"PhoBERT\")\n","\n","‚ö° L∆∞u √Ω:\n","- Dataset ƒë∆∞·ª£c load ƒë√∫ng nh∆∞ code g·ªëc c·ªßa b·∫°n\n","- C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh sample_size trong CELL 13\n","- S·ª≠ d·ª•ng GPU runtime ƒë·ªÉ train nhanh h∆°n\n","\"\"\")"],"metadata":{"id":"UH0WuzqmDcFE","executionInfo":{"status":"aborted","timestamp":1757172363537,"user_tz":-420,"elapsed":4,"user":{"displayName":"L√ÇM ANH KI·ªÜT","userId":"03787920270525553969"}}},"execution_count":null,"outputs":[]}]}